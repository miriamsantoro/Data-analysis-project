
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Relazione analisi dati}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Confronto di classificatori per il Dataset
WDBC}\label{confronto-di-classificatori-per-il-dataset-wdbc}

\subsection{Elaborato per il corso di Analisi
Dati}\label{elaborato-per-il-corso-di-analisi-dati}

\subsubsection{Miriam Santoro}\label{miriam-santoro}

\paragraph{aa. 2017/2018, Università di Bologna - Corso di laurea
magistrale in Fisica
Applicata}\label{aa.-20172018-universituxe0-di-bologna---corso-di-laurea-magistrale-in-fisica-applicata}

    \subsection{Scopo del progetto}\label{scopo-del-progetto}

    Il presente lavoro è stato realizzato in linguaggio \texttt{Python3} e
si pone come obiettivo l'implementazione e la valutazione di: 1. 10
classificatori creati utilizzando le funzioni della libreria
\texttt{sklearn}, 2. un classificatore basato su una rete neurale
costruito tramite la libreria \texttt{pytorch}.

Il progetto ha previsto anche l'implementazioni di altre librerie, oltre
quelle già menzionate, quali: 1. \texttt{numpy} 2. \texttt{pandas} 3.
\texttt{matplotlib} 4. \texttt{scipy} 5. \texttt{graphviz}

    \subsection{Esecuzione del progetto}\label{esecuzione-del-progetto}

    I classificatori sono stati implementati in due script diversi a seconda
della libreria utilizzata. Nello specifico: 1.
\texttt{Classification.py} contiene i classificatori definiti usando
\texttt{sklearn}; nello specifico: 1. 10 classificatori implementati con
possibilità di scegliere le features; 2. Gli stessi 10 classificatori
implementati tenendo conto delle 10 features migliori. 2.
\texttt{NNC.py} contiene il classificatore definito usando Pytorch;
nello specifico: 1. Il classificatore implementato con possibilità di
scegliere le features 2. Lo stesso classificatore implementato tenendo
conto delle 10 features migliori

Questi due file Python sono stati implementati in \texttt{main.py},
script che è necessario eseguire per ottenere i risultati. Inoltre, in
\texttt{main.py} è stato importato anche lo script \texttt{Plotting.py},
utile per visualizzare i vari plot che verranno descritti nell'apposita
sezione \textbf{Plotting}.

    \subsection{Dataset (note preliminari)}\label{dataset-note-preliminari}

    Il WDBC \emph{Wisconsis Diagnostic Breast Cancer} è un dataset
contenente 569 istanze, corrispondenti a 569 pazienti, ognuno dei quali
classificato tramite un Id-paziente, un lettera indicante il tipo di
tumore (maligno o benigno) e 30 features. In particolare, le features
legate al tumore indicano, in ordine: 1. Raggio (media delle distanze
dal centro ai punti sul perimetro) 2. Texture (deviazione standard dei
valori in scala di grigi) 3. Perimetro 4. Area 5. Smoothness (variazione
locale nelle lunghezze del raggio) 6. Compattezza (perimetro\^{}2/area
-1.0) 7. Concavità (gravità delle porzioni concave del contorno) 8.
Punti concavi (numeri di porzioni concave del contorno) 9. Simmetria 10.
Dimensioni frattali ("approssimazione coastline" -1)

Le features sono state calcolate da un'immagine digitalizzata di un fine
aspiratore (FNA) di una massa del seno e descrivono le caratteristiche
dei nuclei cellulari nell'immagine. Nello specifico per ogni immagine si
hanno 30 features corrispondenti alla media, all'errore stardard e alla
"peggiore" o più grande di ogni misura nell'elenco puntato e ogni cifra
è stata acquisita con 4 digits.

Inoltre, è necessario aggiungere che le 569 istanze possono essere
divise in due classi, la prima avente 357 tumori benigni e la seconda
avente 212 tumori maligni.

Una volta estratti i dati tramite la libreria \texttt{pandas} e aver
disposto features e labels in array si è utilizzata la funzione
\texttt{model\_selection.train\_test\_split} contenuta in
\texttt{sklearn} per splittare in maniera diversa il dataset in training
set e test set. Questi ultimi sono stati divisi come segue: 1. 90\%
training, 10\% test 2. 80\% training, 20\% test 3. 50\% training, 50\%
test 4. 25\% training, 75\% test

Le divisioni sono state pensate in modo da evitare l'overfitting e
valutare in varie condizioni le performance di tutti i classificatori
utilizzati. Le diverse divisioni sono state implementate tramite un
ciclo for all'interno di tutti i classificatori, come è mostrato nelle
seguenti righe di codice:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{seq }\OperatorTok{=}\NormalTok{ [.}\DecValTok{9}\NormalTok{, .}\DecValTok{8}\NormalTok{, .}\DecValTok{5}\NormalTok{, .}\DecValTok{25}\NormalTok{]}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in}\NormalTok{ seq:}
\NormalTok{    X_train, X_test, Y_train, Y_test }\OperatorTok{=}\NormalTok{ model_selection.train_test_split(X, Y, train_size}\OperatorTok{=}\NormalTok{i, test_size}\OperatorTok{=} \DecValTok{1}\OperatorTok{-}\NormalTok{i, }
\NormalTok{                                                                        random_state}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Per quanto riguarda il classificatore implementato tramite la libreria
\texttt{pytorch} è stato necessario sfuttare il one hot encoding, un
processo tramite cui è possibile convertire le labels in numeri binari.
In questo caso, quindi, sono stati associati \([1,0]\) e \([0,1]\) alle
due possibili uscite (rispettivamente \(B\) e \(M\)) e questo passaggio
è risultato indispensabile in quanto \texttt{pytorch} è in grado di
funzionare solo su dati numerici.

    \subsection{Analisi dei classificatori
utilizzati}\label{analisi-dei-classificatori-utilizzati}

    Si analizzano e commentano di seguito i classificatori utilizzati in
questo progetto.

    \subsubsection{\texorpdfstring{Classificatori da
\texttt{sklearn}}{Classificatori da sklearn}}\label{classificatori-da-sklearn}

    I classificatori implementati grazie \texttt{sklearn} sono i seguenti:
1. LogReg (\emph{Logistic Regression classifier}) 2. SVM (\emph{Support
Vector Machine classifier}) 3. DTC (\emph{Decision Tree Classifier}) 4.
KNC (\emph{K Neighbor Classifier}) 5. RFC (\emph{Random Forest
Classifier}) 6. MLP (\emph{Multi Layer Perceptron classifier}) 7. ABC
(\emph{Ada Boost Classifier}) 8. GNB (\emph{Gaussian Naive Bayes
classifier}) 9. QDA (\emph{Quadratic Discriminant Analysis classifier})
10. SGD (\emph{Stochastic Gradient Descent classifier})

In un primo momento, ciascuno di questi classificatori è stato eseguito
e valutato al variare del numero di features. Nello specifico sono state
prese in considerazione le seguenti casistiche: 1. 1 features 2. 9
features 3. 16 features 4. 30 features

Per fare ciò è stato solo necessario cambiare i parametri d'ingresso
alle funzioni utilizzate per ciascun classificatore, come è possibile
notare nel seguente esempio di codice:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#da main.py}
\NormalTok{l1 }\OperatorTok{=}\NormalTok{ Classification.LogReg(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{)}
\NormalTok{m1 }\OperatorTok{=}\NormalTok{ Classification.LogReg(}\DecValTok{2}\NormalTok{,}\DecValTok{9}\NormalTok{)}
\NormalTok{n1 }\OperatorTok{=}\NormalTok{ Classification.LogReg(}\DecValTok{2}\NormalTok{,}\DecValTok{16}\NormalTok{)}
\NormalTok{o1 }\OperatorTok{=}\NormalTok{ Classification.LogReg(}\DecValTok{2}\NormalTok{,}\DecValTok{30}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Inoltre, dopo che, tramite la funzione \texttt{stats.spearmanr},
implementata all'interno di \texttt{Histo} in \texttt{Plotting.py}, sono
state individuate le 10 features più correlate alle labels, sono stati
realizzati classificatori uguali a quelli precedenti ma in modo che
prendessero come input queste 10 specifiche features. Per fare ciò, si è
aggiunto un 10 al nome delle funzioni precedenti usate per i
classificatori e non sono stati forniti parametri di input
nell'argomento della funzione. La chiamata a uno di questi
classificatori è mostrata nella seguente riga di codice, tratta dal
main:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 }\OperatorTok{=}\NormalTok{ Classification.LogReg10()}
\end{Highlighting}
\end{Shaded}

    \paragraph{1. LogReg}\label{logreg}

    Si usa l'implementazione standard del classificatore Logistic Regression
contenuto nella libreria \texttt{sklearn.linear\_model}. Questo è usato
per valutare i parametri di un modello statistico al fine di modellare
una variabile dipendente binaria, ovvero una variabile con due possibili
valori (nel nostro caso etichettati con "B"=0 e "M"=1). La funzione
utilizzata in questo modello è chiamata logistica ed è una funzione del
tipo:

\(\begin{equation*} f(x)={\frac {L}{1+e^{-k(x-x_{0})}}}\\ \end{equation*}\)

dove: - e = numero di Eulero, - x0 = valore sull'asse delle x del punto
a metà della funzione sigmoide, - L = massimo valore della curva - k =
ripidezza della curva.

Nello specifico, per quanto riguarda il classificatore usato in questo
progetto e mostrato di seguito:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#da Classification.py}
\NormalTok{cl }\OperatorTok{=}\NormalTok{ linear_model.LogisticRegression(C}\OperatorTok{=}\FloatTok{2.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

si sono usati: - \(C=2.5\) come inverso della forza di regolarizzazione,
indicante la tolleranza di casi mal classificati, in quanto questo
valore è poco al di sopra del valore di default 1 ed è utile per evitare
errori di overfitting; - altri parametri lasciati di default tra cui: -
\(\text{solver='liblinear'}\) come risolutore, in quanto per piccoli
dataset è una buona scelta - \(\text{penalty='l2'}\) come termine di
penalizzazione

In seguito all'addestramento del classificatore tramite la funzione
\texttt{fit}, viene utilizzata la funzione \texttt{predict} per fare
previsioni sui dati di test e vengono calcolati rispettivamente report
di classificazione, matrice di confusione e accuratezza.

    \paragraph{2. SVM}\label{svm}

    Si usa l'implementazione standard della Support Vector Classification
contenuta nella libreria \texttt{sklearn.svm}. Questo classificatore è
detto anche classificatore a massimo margine perchè allo stesso tempo
minimizza l'errore empirico di classificazione e massimizza il margine
geometrico. Nello specifico, cerca gli iperpiani di separazioni ottimali
tramite una funzione obiettivo senza minimi locali.

Tra i vantaggi nell'uso del SVC possiamo trovare il fatto che: - sia
efficiente dal punto di vista della memoria in quanto usa un subset di
punti di training nella funzione di decisione (chiamati support vectors)
- sia versatile poichè per come funzioni di decisione si possono
specificare diverse funzioni Kernel. Queste permettono di proiettare il
problema iniziale su uno spazio di dimensioni superiore senza grandi
costi computazionali e ottenendo separazioni basate anche su superfici
non lineari.

Nello specifico, per quanto riguarda il classificatore usato in questo
progetto e mostrato di seguito:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#da Classification.py}
\NormalTok{cl}\OperatorTok{=}\NormalTok{svm.SVC(kernel}\OperatorTok{=}\StringTok{'linear'}\NormalTok{)    }
\end{Highlighting}
\end{Shaded}

si sono usati: - come funzione kernel, una funzione lineare del tipo:
\(\begin{equation*}  \langle x, x'\rangle  \end{equation*}\) - altri
parametri di default, tra cui: - \(C=1.0\) come parametro di penalità
per il termine di errore

In seguito all'addestramento del classificatore tramite la funzione
\texttt{fit}, viene utilizzata la funzione \texttt{predict} per fare
previsioni sui dati di test e vengono calcolati rispettivamente report
di classificazione, matrice di confusione e accuratezza

    \paragraph{3. DTC}\label{dtc}

    Si usa l'implementazione standard del Decision Tree Classifier contenuto
nella libreria \texttt{sklearn.tree}. Questo classificatore è usato per
valutare i parametri di un modello al fine di predire il valore di un
target variabile, apprendendo semplici regole di decisione dedotte dalle
features.

Tra i vantaggi di questo classificatore è possible trovare il fatto che:
- sia semplice da capire ed interpretare in quanto si basa su un modello
white box in cui l'interpretazione di una condizione è facilmente
spiegata dalla logica booleana - gli alberi (\emph{trees}) possano
essere visualizzati - il costo di predire i dati usando l'albero sia
logaritmico nel numero di punti di dati usati per allenare l'albero
stesso.

Nello specifico, per quanto riguarda il classificatore implementato in
questo progetto e mostrato di seguito:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#da Classification.py}
\NormalTok{cl }\OperatorTok{=}\NormalTok{ tree.DecisionTreeClassifier()      }
\end{Highlighting}
\end{Shaded}

si sono usati i parametri di default, tra cui: -
\(\text{criterion='gini'}\) come funzione per misurare la qualità dello
split; - \(\text{splitter='best'}\) come strategia usata per scegliere
lo split ad ogni nodo in modo da scegliere quello migliore; -
\(\text{max_features=None}\) come numero di features da considerare
quando si guarda allo split migliore. In questo caso
max\_features=n\_features; - \(\text{max_depth=None}\) come massima
profondità dell'albero None. Questo significa che i nodi vengono estesi
fin quando tutte le foglie sono pure.

Inoltre, l'equazione di diminuzione di impurità pesata (che governa lo
split) è:

\(\begin{equation*} \frac{N_t}{N} \cdot \left(\text{impurity} - \frac{N_{t_R}}{N_t} \cdot \text{right_impurity} - \frac{N_{t_L}}{N_t} \cdot \text{left_impurity} \right) \\ \end{equation*}\)

dove: - \(N\) è il numero totale di campioni - \(N_t\) è il numero di
campioni al nodo corrente - \(N_{t_L}\) è il numero di campioni nella
parte destra - \(N_{t_R}\) è il numero di campioni nella parte sinistra

In seguito all'addestramento del classificatore tramite la funzione
\texttt{fit}, viene utilizzata la funzione \texttt{predict} per fare
previsioni sui dati di test e vengono calcolati rispettivamente report
di classificazione, matrice di confusione e accuratezza.

    \paragraph{4. KNC}\label{knc}

    Si usa l'implementazione standard del K Neighbors Classifier contenuto
nella libreria \texttt{sklearn.neighbors}. Questo classificatore non
cerca di costruire un modello interno generale, ma semplicemente
memorizza le istanze dei dati di training; quindi la classificazione è
calcolata da un semplice voto di maggioranza dei vicini più vicini ad
ogni punto: il punto di ricerca è assegnato alla classe che ha il
maggior numero di rappresentanti nei vicini più vicini del punto.

Questo classificatore, quindi, si basa su k vicini, dove k è un valore
intero specificato dall'utente e la sua scelta ottimale dipende
fortemente dai dati. Ad esempio, in generale, una k più grande sopprime
gli effetti del rumore ma rende i confini di classificazione meno
distinti.

Nello specifico, per quanto riguarda il classificatore implementato in
questo progetto e mostrato di seguito:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#da Classification.py}
\NormalTok{cl }\OperatorTok{=}\NormalTok{ neighbors.KNeighborsClassifier(n_neighbors}\OperatorTok{=}\DecValTok{3}\NormalTok{)       }
\end{Highlighting}
\end{Shaded}

si sono usati: - \(\text{n_neighbors=3}\) come numero di vicini; - altri
parametri di default tra cui: - \(\text{weights='uniform'}\) come
funzione per i pesi usata per la previsione. I pesi uniformi portano a
pesare equamente tutti i punti in ogni vicinato -
\(\text{algorithm='auto'}\) come algoritmo usato per calcolare i vicini.
Questo è l'algoritmo più appropriato sulla base dei valori passati dal
metodo di fit. - \(\text{metric='minkowski'}\) come metrica ovvero
distanza usata per l'albero.

In seguito all'addestramento del classificatore tramite la funzione
\texttt{fit}, viene utilizzata la funzione \texttt{predict} per fare
previsioni sui dati di test e vengono calcolati rispettivamente report
di classificazione, matrice di confusione e accuratezza.

    \paragraph{5. RFC}\label{rfc}

    Si usa l'implementazione standard del Random Forest Classifier contenuto
nella libreria \texttt{sklearn.ensemble}. Questo classificare fa il fit
di un numero di classificatori decision trees su vari sotto-campioni del
dataset e usa la media per migliorare l'accuratezza predittiva e
controllare l'over-fitting. La grandezza dei sotto campioni è uguale a
quella del campione di input iniziale ma i campioni sono disegnati con
rimpiazzamento dal set di training. Inoltre, quando si fa la divisione
di un nodo durante la costruzione dell'albero, lo split che viene scelto
non è il migliore tra tutte le features ma tra un subset random di
features. A causa di questa randomicità, il bias della foresta dovrebbe
aumentare (rispetto a quello di un singolo albero non-random) ma, grazie
alla media, la sua varianza diminuisce e compensa di più l'aumento del
bias determinando un modello migliore.

Nello specifico, per quanto riguarda il classificatore usato in questo
progetto e mostrato di seguito:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#da Classification.py}
\NormalTok{cl}\OperatorTok{=}\NormalTok{ensemble.RandomForestClassifier(max_depth}\OperatorTok{=}\DecValTok{15}\NormalTok{, n_estimators}\OperatorTok{=}\DecValTok{10}\NormalTok{, max_features}\OperatorTok{=}\DecValTok{1}\NormalTok{)      }
\end{Highlighting}
\end{Shaded}

si sono usati: - \(\text{max_depth=15}\) come massima espansione
dell'albero; - \(\text{n_estimators=10}\) come numero di alberi
(estimatori) nella foresta; - \(\text{max_features=1}\) come numero di
features da considerare quando si guarda al migliore split; - altri
parametri di default, tra cui: - \(\text{criterion='gini'}\) come
funzione per misurare la qualità dello split; -
\(\text{min_samples_split=2}\) come numero minimo di campioni richiesto
per dividere un nodo interno.

In seguito all'addestramento del classificatore tramite la funzione
\texttt{fit}, viene utilizzata la funzione \texttt{predict} per fare
previsioni sui dati di test e vengono calcolati rispettivamente report
di classificazione, matrice di confusione e accuratezza.

    \paragraph{6. MLP}\label{mlp}

    Si usa l'implementazione standard del Multi Layer Perceptron contenuto
nella libreria \texttt{sklearn.neural\_network}. Questo classificatore
si basa su una rete neurale e su un allenamento iterativo. Infatti, ad
ogni step temporale vengono calcolate le derivate parziali della
funzione di perdita rispetto ai parametri del modello per aggiornare i
parametri stessi. Può avere anche un termine di regolarizzazione
aggiunto alla funzione di perdita che restringe i parametri del modello
per prevenire l'overfitting.

Nello specifico, per quanto riguarda il classificatore utilizzato in
questo progetto e mostrato di seguito:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#da Classification.py}
\NormalTok{cl }\OperatorTok{=}\NormalTok{ neural_network.MLPClassifier(activation}\OperatorTok{=}\StringTok{'logistic'}\NormalTok{, solver}\OperatorTok{=}\StringTok{'lbfgs'}\NormalTok{, max_iter}\OperatorTok{=}\DecValTok{1000}\NormalTok{ )       }
\end{Highlighting}
\end{Shaded}

si sono usati: - \(\text{activation='logistic'}\) come funzione di
attivazione per gli strati nascosti. Questa è una funzione logistica
sigmoidale che restituisce:

\begin{verbatim}
$\begin{equation*}
f(x) = \frac{1}{1+exp(-x)}\\
\end{equation*}$
\end{verbatim}

\begin{itemize}
\tightlist
\item
  \(\text{solver='lbfgs'}\) come risolutore per l'ottimizzazione dei
  pesi. Questo è un ottimizzatore facente parte dei metodi
  quasi-newtoniani ed è stato scelto in quanto per piccoli datasets,
  come il wdbc, converge più velocemente e ha migliori performance;
\item
  \(\text{max_iter=1000}\) come massimo numero di iterazioni per la
  convergenza;
\item
  altri parametri di default, tra cui:

  \begin{itemize}
  \tightlist
  \item
    \(\text{hidden_layer_sizes=(100,)}\) come dimensioni degli strati
    nascosti;
  \item
    \(\text{alpha=0.0001}\) come parametro di penalizzazione L2;
  \item
    \(\text{batch_size=min(200,n_samples)}\) come grandezza di batch;
  \item
    \(\text{learning_rate=costant=0.001}\) come frequenza di
    apprendimento per l'aggiornamento dei pesi.
  \end{itemize}
\end{itemize}

In seguito all'addestramento del classificatore tramite la funzione
\texttt{fit}, viene utilizzata la funzione \texttt{predict} per fare
previsioni sui dati di test e vengono calcolati rispettivamente report
di classificazione, matrice di confusione e accuratezza.

    \paragraph{7. ABC}\label{abc}

    Si usa l'implementazione standard dell'Ada Boost Classifier contenuto
nella libreria \texttt{sklearn.ensemble}. Questo classificatore
inizialmente fa il fit di un modello debole (leggermente migliori di
quelli random) sul dataset e poi fitta copie aggiuntive dello stesso
modello sullo stesso dataset ma aggiustando i pesi di istanze di
training classificate non correttamente in modo che i classificatori
seguenti si focalizzino di più su casi difficili.

Iniziamente, questi pesi sono tutti settati a 1/N, così che il primo
step alleni semplicemente un debole modello dei dati originali;
tuttavia, per ogni iterazione successiva, sono modificati
individualmente in modo che: - i pesi associati agli esempi di training
che non sono predetti correttamente vengano aumentati; - i pesi
associati agli esempi di training che sono predetti correttamente
vengano diminuiti.

Successivamente l'algoritmo di apprendimento viene riapplicato per
ripesare i dati. In questo modo ogni volta che le iterazioni procedono,
gli esempi difficili da predire ricevono un'influenza sempre crescente e
ogni modello debole viene forzato a concentrarsi sugli esempi che
mancano dai precedenti nella sequenza. Infine, le previsioni da ciascuno
di loro vengono combinate attraverso un voto di maggioranza pesata per
produrre la previsione finale.

Nello specifico, per quanto riguarda il classificatore usato in questo
progetto e mostrato di seguito:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#da Classification.py}
\NormalTok{cl}\OperatorTok{=}\NormalTok{ensemble.AdaBoostClassifier()       }
\end{Highlighting}
\end{Shaded}

si sono usati i parametri di default, tra cui: -
\(\text{base_estimator=None=DecisionTreeClassifier(max_depth=1)}\) come
estimatore base da cui è costruito l'ensemble potenziato; -
\(\text{n_estimators=50}\) come numero di estimatori a cui viene
concluso il potenziamento; - \(\text{learning_rate=1}\) come frequenza
di apprendimento; - \(\text{algorithm='SAMME.R'}\) come algoritmo di
potenziamento. Questo converge velocemente, raggiungendo un errore
minore di testing con minori iterazioni di boosting.

In seguito all'addestramento del classificatore tramite la funzione
\texttt{fit}, viene utilizzata la funzione \texttt{predict} per fare
previsioni sui dati di test e vengono calcolati rispettivamente report
di classificazione, matrice di confusione e accuratezza.

    \paragraph{8. GNB}\label{gnb}

    Si usa l'implementazione standard del Gaussian Naive Bayes contenuto
nella libreria \texttt{sklearn.naive\_bayes}. Questo classificatore si
basa sull'applicazione del teorema di Bayes, con l'assunzione di una
forte (naive) indipendenza condizionata tra ogni coppia di features,
dato il valore della variabile di classe. Il teorema di Bayes stabilisce
che, data la variabile di classe \(y\) e il vettore di feature
dipendente \(x_1\) attraverso \(x_n\), si ha:

\(\begin{equation*} P(y \mid x_1, \dots, x_n) = \frac{P(y) P(x_1, \dots x_n \mid y)} {P(x_1, \dots, x_n)} \\ \end{equation*}\)

L'assunzione di indipendenza naive condizionale, invece, stabilisce che:

\(\begin{equation*} P(x_i | y, x_1, \dots, x_{i-1}, x_{i+1}, \dots, x_n) = P(x_i | y) \\ \end{equation*}\)

Questo classificatore implementa l'algoritmo Gaussian Naive Bayes per la
classificazione in cui si assume che la likelihood delle features sia
gaussiana:
\(\begin{equation*} P(x_i \mid y) = \frac{1}{\sqrt{2\pi\sigma^2_y}} \exp\left(-\frac{(x_i - \mu_y)^2}{2\sigma^2_y}\right) \\ \end{equation*}\)

dove i parametri \(\sigma_y\) e \(\mu_y\) sono stimati usando il massimo
della likelihood.

Tra i vantaggi di questo classificatore troviamo il fatto che: -
richiede una piccola quantità di dati di training per stimare i
parametri necessari; - può essere estremamente veloce se confrontato con
metodi più sofisticati; - il disaccoppiamento delle distribuzioni di
features condizionali delle classi può essere stimato indipendentemente
come una distribuzione unidimensionale e questo aiuta ad ridurre i
problemi che derivano dalla 'maledizione' della dimensionalità.

Nello specifico, per quanto riguarda il classificatore usato in questo
progetto e mostrato di seguito:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#da Classification.py}
\NormalTok{cl}\OperatorTok{=}\NormalTok{naive_bayes.GaussianNB()    }
\end{Highlighting}
\end{Shaded}

in cui si sono utilizzati i parametri di default, ovvero: -
\(\text{priors=(n_classes,)}\) come probabilità a priori delle classi; -
\(\text{var_smoothing=1e-9}\) come porzione della varianza più grande
tra tutte le features. Questa viene aggiunta alle varianze per il
calcolo della stabilità.

In seguito all'addestramento del classificatore tramite la funzione
\texttt{fit}, viene utilizzata la funzione \texttt{predict} per fare
previsioni sui dati di test e vengono calcolati rispettivamente report
di classificazione, matrice di confusione e accuratezza.

    \paragraph{9. QDA}\label{qda}

    Si usa l'implementazione standard della Quadratic Discriminant Analysis
contenuta nella libreria \texttt{sklearn.discriminant\_analysis}. Questo
classificatore può essere ottenuto da semplici modelli probabilistici
che modellano la distribuzione condizionale di classe dei dati
\(P(X|y=k)\) per ogni classe \(k\). Le previsioni possono essere
ottenute usando la regola di Bayes:

\(\begin{equation*} P(y=k | X) = \frac{P(X | y=k) P(y=k)}{P(X)} \\ \end{equation*}\)

Nello specifico, \(P(X|y)\) è modellizzato come una distribuzione
Gaussiana multivariata con densità:

\(\begin{equation*} P(X | y=k) = \frac{1}{(2\pi)^{d/2} |\Sigma_k|^{1/2}}\exp\left(-\frac{1}{2} (X-\mu_k)^t \Sigma_k^{-1} (X-\mu_k)\right) \\ \end{equation*}\)

dove \(d\) è il numero di features.

In poche parole, questo è un classificatore con un limite di decisione
(superficie di separazione) quadratico che è generato fittando le
densità condizionali delle classi e usando la regola di Bayes. Il
modello fitta una densità Gaussiana ad ogni classe.

Nello specifico, per quanto riguarda il classificatore usato in questo
progetto e mostrato di seguito:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#da Classification.py}
\NormalTok{cl}\OperatorTok{=}\NormalTok{discriminant_analysis.QuadraticDiscriminantAnalysis()      }
\end{Highlighting}
\end{Shaded}

sono stati usati come parametri quelli di default, tra cui: -
\(\text{priors=n_classes}\) come priori sulle classi; -
\(\text{tol=1.0e-4}\) come soglia usata per la stima del rango.

In seguito all'addestramento del classificatore tramite la funzione
\texttt{fit}, viene utilizzata la funzione \texttt{predict} per fare
previsioni sui dati di test e vengono calcolati rispettivamente report
di classificazione, matrice di confusione e accuratezza.

    \paragraph{10. SGD}\label{sgd}

    Si usa l'implementazione standard della Stochastic Gradient Descent
contenuta nella libreria \texttt{sklearn.linear\_model}. Questo è un
classificatore lineare con apprendimento tramite il gradiente di discesa
stocastica (SGD); nello specifico, questo implica che per ogni campione:
- viene stimato il gradiente di perdita; - viene aggiornato il modello
man mano con una frequenza di apprendimento decrescente.

Il regolarizzatore è un termine di penalità che viene aggiunto alla
funzione di perdita e che restringe i parametri del modello.

Nello specifico, per quanto riguarda il classificatore usato in questo
progetto e mostrato di seguito:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#da Classification.py}
\NormalTok{cl }\OperatorTok{=}\NormalTok{ linear_model.SGDClassifier(loss}\OperatorTok{=}\StringTok{"perceptron"}\NormalTok{, penalty}\OperatorTok{=}\StringTok{"elasticnet"}\NormalTok{, max_iter}\OperatorTok{=}\DecValTok{600}\NormalTok{)   }
\end{Highlighting}
\end{Shaded}

si sono usati: - \(\text{loss='perceptron'}\) come funzione di perdita,
ovvero come perdita lineare usata dall'algoritmo del percettrone; -
\(\text{penalty='elasticnet'}\) come penalità (termine di
regolarizzazione). Questo è un termine che viene aggiunto alla funzione
di perdita, restringe i parametri del modello e, in questo caso, è una
combinazione di L2 (norma euclidea quadrata) e L1 (norma assoluta); -
\(\text{max_iter=600}\) come massimo numero di passi sui dati di
training; - altri parametri di default, tra cui: - \(\text{tol=1e-3}\)
come criterio di stop.

In seguito all'addestramento del classificatore tramite la funzione
\texttt{fit}, viene utilizzata la funzione \texttt{predict} per fare
previsioni sui dati di test e vengono calcolati rispettivamente report
di classificazione, matrice di confusione e accuratezza.

    \subsubsection{\texorpdfstring{Classificatore da
\texttt{pytorch}}{Classificatore da pytorch}}\label{classificatore-da-pytorch}

    \paragraph{NNC}\label{nnc}

    Per l'implementazione della CNN è stata utilizzata la libreria
\texttt{pytorch} che sfrutta una sintassi basata su tensori e
programmazione ad oggetti.

Come struttura della rete neurale, si sono utilizzati: 1. un livello
lineare che va da e-s a 15 canali, dove s ed e sono rispettivamente la
prima e l'ultima feature considerate; 2. un livello di passaggio per la
funzione ReLU, utile per scartare i valori negativi ottenuti finora; 3.
un livello lineare che va da 15 a 10 canali; 4. un livello di passaggio
per la funzione ReLU, avente lo stesso scopo del livello 2; 5. un
livello finale lineare che va da 10 a 2 canali.

Per l'addestramento della CNN si sottopone alla rete neurale l'intero
dataset mescolato più volte, dove il numero di "mescolamenti" è detto
\texttt{epoch}.

Come funzione di errore si è scelta \texttt{SmoothL1Loss} perchè
ritenuta adeguata per una classificazione 2-classi e perchè ritenuta
migliore rispetto alle altre funzioni di errore candidate. Nello
specifico, questa crea un criterio che usa un termine quadrato se
l'errore assoluto element-wise cade al di sotto dell'1 e un termine L1
altrimenti. Inoltre, è meno sensibile ai valori anomali rispetto al
MSELoss ed è conosciuta anche come Huber loss:

\(\begin{equation*} \text{loss}(x, y) = \frac{1}{n} \sum_{i} z_{i} \\ \end{equation*}\)

dove \(z_i\) è data da:

\(\begin{equation*} \begin{split}z_{i} = \begin{cases} 0.5 (x_i - y_i)^2, & \text{se } |x_i - y_i| < 1 \\ |x_i - y_i| - 0.5, & \text{altrimenti } \end{cases}\end{split} \end{equation*}\)

Insieme a questa è stata usata come funzione di ottimizzazione
\texttt{torch.optim.Adam}, una funzione adattiva dotata di gradient
descent e suggerita dalla documentazione di \texttt{pytorch} per
l'addestramento. Durante l'addestramento si calcola quindi la
backpropagation dell'errore lungo tutta la rete e si esegue uno step di
ottimizzazione la cui dimensione è definita dal
\texttt{learning\_rate}(=0.005, in questo caso) nella funzione
\texttt{torch.optim.Adam}.

Per quanto riguarda il testing e l'accuratezza delle previsioni fatte
dalla rete neurale si sono usate le funzioni \texttt{np.argmax}
contenute nella libreria \texttt{sklearn}.

Di seguito, si riporta lo script della rete neurale, comprendente
addestramento e testing:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#da NNC.py}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}
\ImportTok{import}\NormalTok{ torch.utils.data}
\ImportTok{import}\NormalTok{ torch.optim }\ImportTok{as}\NormalTok{ optim}
\ImportTok{from}\NormalTok{ torch.autograd }\ImportTok{import}\NormalTok{ Variable}
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ model_selection}
\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ LabelEncoder, OneHotEncoder}

\KeywordTok{def}\NormalTok{ NNC(s,e):}
\NormalTok{    myList }\OperatorTok{=}\NormalTok{ []}
\NormalTok{    f}\OperatorTok{=}\BuiltInTok{open}\NormalTok{(}\StringTok{"wdbc.data.txt"}\NormalTok{)}
\NormalTok{    data_train }\OperatorTok{=}\NormalTok{ pd.read_csv(}\StringTok{"wdbc.data.txt"}\NormalTok{, header}\OperatorTok{=}\VariableTok{None}\NormalTok{, sep}\OperatorTok{=}\VerbatimStringTok{r"\textbackslash{}s+"}\NormalTok{)}
\NormalTok{    X }\OperatorTok{=}\NormalTok{ np.array(data_train)}
    
\NormalTok{    pre_lab}\OperatorTok{=}\NormalTok{X[:,}\DecValTok{1}\NormalTok{]  }
\NormalTok{    features}\OperatorTok{=}\NormalTok{X[:,s:e]}
    
\NormalTok{    pre_labels}\OperatorTok{=}\NormalTok{[]}
\NormalTok{    label_encoder }\OperatorTok{=}\NormalTok{ LabelEncoder()                                  }
\NormalTok{    pre_labels }\OperatorTok{=}\NormalTok{ label_encoder.fit_transform(pre_lab)}
\NormalTok{    pre_labels }\OperatorTok{=}\NormalTok{ np.array(pre_labels)}
    
\NormalTok{    labels}\OperatorTok{=}\NormalTok{[]}
    \ControlFlowTok{for}\NormalTok{ num }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(pre_labels)):}
        \ControlFlowTok{if}\NormalTok{ pre_labels[num] }\OperatorTok{==} \DecValTok{0}\NormalTok{:}
\NormalTok{            labels.append([}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{])}
        \ControlFlowTok{if}\NormalTok{ pre_labels[num] }\OperatorTok{==} \DecValTok{1}\NormalTok{:}
\NormalTok{            labels.append([}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{])}
\NormalTok{    labels }\OperatorTok{=}\NormalTok{ np.array(labels)}
    
\NormalTok{    seq}\OperatorTok{=}\NormalTok{[.}\DecValTok{9}\NormalTok{, .}\DecValTok{8}\NormalTok{, .}\DecValTok{5}\NormalTok{, .}\DecValTok{25}\NormalTok{]}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in}\NormalTok{ seq:}

\NormalTok{        features_train, features_test, labels_train, labels_test }\OperatorTok{=}\NormalTok{ model_selection.train_test_split(features, }
\NormalTok{                                                                                                    labels, train_size}\OperatorTok{=}\NormalTok{i,}
\NormalTok{                                                                                                    test_size}\OperatorTok{=}\DecValTok{1}\OperatorTok{-}\NormalTok{i,}
\NormalTok{                                                                                                    random_state}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{        features_train }\OperatorTok{=}\NormalTok{ np.array(features_train, dtype}\OperatorTok{=}\NormalTok{np.float32)}
\NormalTok{        features_test }\OperatorTok{=}\NormalTok{ np.array(features_test, dtype}\OperatorTok{=}\NormalTok{np.float32)}
\NormalTok{        labels_train }\OperatorTok{=}\NormalTok{ np.array(labels_train, dtype}\OperatorTok{=}\NormalTok{np.int64)}
\NormalTok{        labels_test }\OperatorTok{=}\NormalTok{ np.array(labels_test, dtype}\OperatorTok{=}\NormalTok{np.int64)}
        
\NormalTok{        features_train_v }\OperatorTok{=}\NormalTok{ Variable(torch.Tensor(features_train), requires_grad }\OperatorTok{=} \VariableTok{False}\NormalTok{)}
\NormalTok{        labels_train_v }\OperatorTok{=}\NormalTok{ Variable(torch.Tensor(labels_train), requires_grad }\OperatorTok{=} \VariableTok{False}\NormalTok{)}
\NormalTok{        features_test_v }\OperatorTok{=}\NormalTok{ Variable(torch.Tensor(features_test), requires_grad }\OperatorTok{=} \VariableTok{False}\NormalTok{)}
\NormalTok{        labels_test_v }\OperatorTok{=}\NormalTok{ Variable(torch.Tensor(labels_test), requires_grad }\OperatorTok{=} \VariableTok{False}\NormalTok{)}
         
\NormalTok{        model }\OperatorTok{=}\NormalTok{ nn.Sequential(nn.Linear(e}\OperatorTok{-}\NormalTok{s, }\DecValTok{15}\NormalTok{),    }
\NormalTok{                     nn.ReLU(),}
\NormalTok{                     nn.Linear(}\DecValTok{15}\NormalTok{, }\DecValTok{10}\NormalTok{),}
\NormalTok{                     nn.ReLU(),}
\NormalTok{                     nn.Linear(}\DecValTok{10}\NormalTok{,}\DecValTok{2}\NormalTok{))}
        
\NormalTok{        loss_fn }\OperatorTok{=}\NormalTok{ torch.nn.SmoothL1Loss()}
\NormalTok{        optim }\OperatorTok{=}\NormalTok{ torch.optim.Adam(model.parameters(), lr}\OperatorTok{=}\FloatTok{0.005}\NormalTok{)}
        
        \CommentTok{#training}
\NormalTok{        all_losses}\OperatorTok{=}\NormalTok{[]}
        \ControlFlowTok{for}\NormalTok{ num }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{2000}\NormalTok{):}
\NormalTok{            optim.zero_grad()                                 }\CommentTok{# Intialize the hidden weights to all zeros}
\NormalTok{            pred }\OperatorTok{=}\NormalTok{ model(features_train_v)                    }\CommentTok{# Forward pass: Compute the output class}
\NormalTok{            loss }\OperatorTok{=}\NormalTok{ loss_fn(pred, labels_train_v)              }\CommentTok{# Compute the loss: difference between the output class and                                                                      the pre-given label}
\NormalTok{            all_losses.append(loss.data)}
\NormalTok{            loss.backward()                                   }\CommentTok{# Backward pass: compute the weight}
\NormalTok{            optim.step()                                      }\CommentTok{# Optimizer: update the weights of hidden nodes}
            
            \BuiltInTok{print}\NormalTok{(}\StringTok{'epoch: '}\NormalTok{, num,}\StringTok{' loss: '}\NormalTok{, loss.item())      }\CommentTok{# Print statistics       }
            
        \CommentTok{#testing}
\NormalTok{        predicted_values}\OperatorTok{=}\NormalTok{[]}
    
        \ControlFlowTok{for}\NormalTok{ num }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(features_test_v)):}
\NormalTok{            predicted_values.append(model(features_test_v[num]))}
        
\NormalTok{        score }\OperatorTok{=} \DecValTok{0}
        \ControlFlowTok{for}\NormalTok{ num }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(predicted_values)):}
            \ControlFlowTok{if}\NormalTok{ np.argmax(labels_test[num]) }\OperatorTok{==}\NormalTok{ np.argmax(predicted_values[num].data.numpy()):}
\NormalTok{                score }\OperatorTok{=}\NormalTok{ score }\OperatorTok{+} \DecValTok{1}     
\NormalTok{        accuracy }\OperatorTok{=} \BuiltInTok{float}\NormalTok{(score }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(predicted_values)) }\OperatorTok{*} \DecValTok{100}
        
        \ControlFlowTok{if}\NormalTok{ accuracy }\OperatorTok{<=} \DecValTok{70}\NormalTok{:}
            \BuiltInTok{print}\NormalTok{ (}\StringTok{"Testing Accuracy Score is: }\SpecialCharTok{%0.2f}\StringTok{ "} \OperatorTok{%}\NormalTok{ (accuracy))}
            \BuiltInTok{print}\NormalTok{(}\StringTok{"Run it again!"}\NormalTok{)}
        \ControlFlowTok{else}\NormalTok{:}
            \BuiltInTok{print}\NormalTok{ (}\StringTok{"Testing Accuracy Score is: }\SpecialCharTok{%0.2f}\StringTok{ "} \OperatorTok{%}\NormalTok{ (accuracy))}
    
\NormalTok{        myList.append(accuracy)}
    
    \BuiltInTok{print}\NormalTok{(myList)    }
    \ControlFlowTok{return}\NormalTok{ myList }
\end{Highlighting}
\end{Shaded}

    \subsubsection{Plotting}\label{plotting}

    Le seguenti funzioni sono implementate in \texttt{Plotting.py}.

    \paragraph{Plot}\label{plot}

    La funzione plot è stata implementata tramite la libreria
\texttt{matplolib}. In questo modo è possibile visualizzare in un plot
bidimensionale la popolazione del dataset iniziale e dei vari splitting
effettuati per ottenere training set e test set. I dati con cui è stato
riempito il plot bidimensionale sono solo legati all'id-paziente e al
tipo di tumore, non alle features. E' stato possibile realizzare questi
grafici tramite la funzione \texttt{scatter}. I risultati sono mostrati
nelle seguenti figure: 

    \paragraph{Histo}\label{histo}

    La funzione è stata implementata tramite la libreria \texttt{matplotlib}
ed è stata creata per visualizzare graficamente, tramite un istogramma,
i risultati riguardanti la correlazione tra le varie features e le
labels del dataset in modo da decretare le 10 features migliori. La
correlazione, non avendo dati distribuiti normalmente, è stata studiata
tramite il coefficiente di Spearman. Infatti, è stata ottenuta tramite
la funzione \texttt{spearmanr} implementata dalla libreria
\texttt{scipy.stats} e restituente valori di correlazione compresi tra
-1 e 1, dove lo 0 indica la non-correlazione. Come è possibile notare
dalla seguente figura, le 10 features classificate come migliori sono
quelle corrispondenti alle colonne: 2,4,5,8,9,15,22,24,25,29 del file
\emph{wdbc.data.txt} (si ricorda che le prime due colonne sono quelle
relative a id-paziente e tipologia del tumore).

    \paragraph{SVC plotting}\label{svc-plotting}

    Tramite la funzione \texttt{SVCPlot(s,e)}, nel caso particolare in cui
si abbiano due features di input, vengono plottati quattro diversi SVC e
la loro rispettiva decision function. I classificatori usati sono i
seguenti:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#da Plotting.py}
\NormalTok{C}\OperatorTok{=}\FloatTok{1.0}  \CommentTok{#parametro di penalità per il termine di errore}
\NormalTok{svc }\OperatorTok{=}\NormalTok{ svm.SVC(kernel}\OperatorTok{=}\StringTok{'linear'}\NormalTok{, C}\OperatorTok{=}\NormalTok{C)}
\NormalTok{rbf_svc }\OperatorTok{=}\NormalTok{ svm.SVC(kernel}\OperatorTok{=}\StringTok{'rbf'}\NormalTok{, gamma}\OperatorTok{=}\FloatTok{0.7}\NormalTok{, C}\OperatorTok{=}\NormalTok{C)}
\NormalTok{rbf_NuSVC }\OperatorTok{=}\NormalTok{ svm.NuSVC(kernel}\OperatorTok{=}\StringTok{'rbf'}\NormalTok{)}
\NormalTok{lin_svc }\OperatorTok{=}\NormalTok{ svm.LinearSVC(C}\OperatorTok{=}\NormalTok{C)}
\end{Highlighting}
\end{Shaded}

dove: 1. svc è il classificatore di cui si è parlato nella sezione
\textbf{SVM}; 2. rbf\_svc è un classificatore avente come kernel una
funzione a base radiale (radial basis function), ovvero del tipo:

\begin{equation*}
    \exp(-\gamma \|x-x'\|^2)
    \end{equation*}

con \(\gamma\) = 0.7 3. rbf\_NuSVC è un classificatore simile a SVC ma
accetta set di parametri leggermente diversi e ha una diversa
formulazione matematica. Nello specifico sfrutta un parametro che
controlla il numero di support vectors (limite inferiore) e gli errori
di allenamento (limite superiore). Anche in questo caso si è utilizzato
come kernel una funzione a base radiale (rbf); 4. lin\_svc è un
classificatore lineare support vector ed è simile al primo
classificatore (svc avente un kernel lineare) ma è implementato tramite
la libreria \texttt{liblinear}, anzichè \texttt{libsvm}. Questo permette
di avere più flessibilità nella scelta dei termini di penalizzazione e
delle funzioni di perdita. I parametri sono lasciati di default e tra
questi è possibile notare: - \(\text{penalty=l2}\) come termine di
penalizzazione; - \(\text{loss='hinge'}\) come funzione di perdita.

Nel caso dell'uso delle due input features mostrate nella seguente riga
di codice:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#da main.py}
\NormalTok{Classification.SVC(}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{)    }
\end{Highlighting}
\end{Shaded}

il risultato che si ottiene è il seguente:

    \paragraph{DTC plotting}\label{dtc-plotting}

    Tramite la funzione \texttt{DTCPlot(s:e)} è possibile ottenere in
formato pdf il DTC generato, al variare dei diversi splitting di
training e test e del numero di features prese in considerazione. In
figura è mostrato un esempio di Tree generato, prendendo in
considerazione 30 features, un training set del 80\% e un test set del
20\%.

    \paragraph{Plot3B}\label{plot3b}

    Tramite la funzione \texttt{Plot3B()} è possibile ottenere un grafico
3-dim avente sui 3 assi le 3 features migliori, dedotte tramite
l'istogramma (\texttt{Histo()}). Come è possibile notare dalla figura,
sembra esserci un andamento lineare sia per i tumori benigni che per
quelli maligni ma nel secondo caso, questo andamento presenta delle
maggiori variazioni. Il risultato è mostrato di seguito: 

    \subsubsection{Esecuzione del programma}\label{esecuzione-del-programma}

    Lo script finale \texttt{main.py} fa uso di tutti i classificatori
(\texttt{Classification.py}, \texttt{NNC.py}) e dello script riguardante
i vari plotting (\texttt{Plotting.py}). Nello specifico: 1. Fa il plot
della popolazione iniziale e dei vari splitting implementati, come già
spiegato nella sezione \textbf{Plot}, tramite la riga di codice:
\texttt{python\ \ \ \ \ \ \ \ \ Plotting.Plot()} 2. Carica ed esegue gli
11 classificatori, tenendo conto di un diverso numero di features (1, 9,
16, 30). Inoltre, ricava grandezze utili per valutare e confrontare i
vari classificatori. Nello specifico: - per ogni classificatore
implementato con \texttt{sklearn} stampa su terminale i risultati
relativi a: 1. matrice di confusione, grazie alla funzione
\texttt{metrics.confusion\_matrix} 2. report di classificazione, grazie
alla funzione \texttt{metrics.classification\_report} 3. accuratezza,
grazie alla funzione \texttt{metrics.accuracy\_score}. - per il
classificatore implementato con \texttt{pytorch} stampa su terminale i
risultati relativi all'accuratezza, grazie alle righe di codice spiegate
nella sezione \textbf{NNC}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  Salva in un DataFrame i risultati relativi alle accuratezze dei vari
  classificatori.
\item
  Fa il plot delle funzioni di decisione per SVC nelle varie casistiche,
  prendendo in considerazione solo due features. Questo è implementato
  tramite la riga di codice:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    Plotting.SVCPlot(s,e)}
\end{Highlighting}
\end{Shaded}
\item
  Permette di ottenere il Decision Tree, di esportarlo e salvarlo in un
  file pdf tramite la riga di codice:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    Plotting.DTCPlot(s,e)}
\end{Highlighting}
\end{Shaded}
\item
  Realizza un istogramma monodimensionale raffigurante i coefficienti di
  correlazione di Spearman per ogni feature, in modo da individuare e
  visualizzare le 10 features migliori, tramite la riga di codice:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    Plotting.Histo()}
\end{Highlighting}
\end{Shaded}
\item
  Carica ed esegue gli 11 classificatori, tenendo conto delle 10
  features migliori. Stampa su terminale le stesse grandezze ottenute
  nel p.to 2.
\item
  Salva in un DataFrame i risultati relativi alle accuratezze dei vari
  classificatori.
\item
  Realizza un plot tridimensionale delle 3 features migliori per
  visualizzarne l'andamento.
\end{enumerate}

    \subsubsection{Risultati e commenti
finali}\label{risultati-e-commenti-finali}

    Una volta fatto girare il programma, i risultati ottenuti per un
training set dell'80\% e un test set del 20\% sono i seguenti:

\textbf{1 feature (sx) e 9 features (dx)}

\textbar{}Classificatore\textbar{}Errori Commessi\textbar{}Percentuale
letta correttamente\textbar{} \textbar{}Classificatore\textbar{}Errori
Commessi\textbar{}Percentuale letta correttamente\textbar{}
\textbar{}:-\/-\/-\textbar{}:-\/-\/-:\textbar{}-\/-:\textbar{}
\textbar{}:-\/-\/-\textbar{}:-\/-\/-:\textbar{}-\/-:\textbar{}
\textbar{}1. LogReg\textbar{}10/114\textbar{}91.23\%\textbar{}
\textbar{}1. LogReg\textbar{}9/114\textbar{}92.11\%\textbar{}
\textbar{}2. SVM\textbar{}9/114\textbar{}92.11\%\textbar{} \textbar{}2.
SVM\textbar{}10/114\textbar{}91.23\%\textbar{} \textbar{}3.
DTC\textbar{}18/114\textbar{}84.21\%\textbar{} \textbar{}3.
DTC\textbar{}8/114\textbar{}92.98\%\textbar{} \textbar{}4.
KNC\textbar{}15/114\textbar{}86.84\%\textbar{} \textbar{}4.
KNC\textbar{}15/114\textbar{}86.84\%\textbar{} \textbar{}5.
RFC\textbar{}19/114\textbar{}83.33\%\textbar{} \textbar{}5.
RFC\textbar{}5/114\textbar{}95.61\%\textbar{} \textbar{}6.
MLP\textbar{}12/114\textbar{}89.47\%\textbar{} \textbar{}6.
MLP\textbar{}10/114\textbar{}91.23\%\textbar{} \textbar{}7.
ABC\textbar{}14/114\textbar{}87.72\%\textbar{} \textbar{}7.
ABC\textbar{}6/114\textbar{}94.74\%\textbar{} \textbar{}8.
GNB\textbar{}12/114\textbar{}89.47\%\textbar{} \textbar{}8.
GNB\textbar{}13/114\textbar{}88.60\%\textbar{} \textbar{}9.
QDA\textbar{}12/114\textbar{}89.47\%\textbar{} \textbar{}9.
QDA\textbar{}6/114\textbar{}94.74\%\textbar{} \textbar{}10.
SGD\textbar{}13/114\textbar{}88.60\%\textbar{} \textbar{}10.
SGD\textbar{}13/114\textbar{}88.60\%\textbar{} \textbar{}11.
NNC\textbar{}15/114\textbar{}86.84\%\textbar{} \textbar{}11.
NNC\textbar{}6/114\textbar{}94.74\%\textbar{}

\textbf{16 features (sx) e 32 features (dx)}

\textbar{}Classificatore\textbar{}Errori Commessi\textbar{}Percentuale
letta correttamente\textbar{} \textbar{}Classificatore\textbar{}Errori
Commessi\textbar{}Percentuale letta correttamente\textbar{}
\textbar{}-\/-\textbar{}:-\/-:\textbar{}-\/-:\textbar{}
\textbar{}-\/-\textbar{}:-\/-:\textbar{}-\/-:\textbar{} \textbar{}1.
LogReg\textbar{}6/114\textbar{}94.74\%\textbar{} \textbar{}1.
LogReg\textbar{}5/114\textbar{}95.61\%\textbar{} \textbar{}2.
SVM\textbar{}8/114\textbar{}92.98\%\textbar{} \textbar{}2.
SVM\textbar{}5/114\textbar{}95.61\%\textbar{} \textbar{}3.
DTC\textbar{}9/114\textbar{}92.11\%\textbar{} \textbar{}3.
DTC\textbar{}10/114\textbar{}91.23\%\textbar{} \textbar{}4.
KNC\textbar{}12/114\textbar{}89.47\%\textbar{} \textbar{}4.
KNC\textbar{}10/114\textbar{}91.23\%\textbar{} \textbar{}5.
RFC\textbar{}8/114\textbar{}92.98\%\textbar{} \textbar{}5.
RFC\textbar{}5/114\textbar{}95.61\%\textbar{} \textbar{}6.
MLP\textbar{}7/114\textbar{}93.86\%\textbar{} \textbar{}6.
MLP\textbar{}11/114\textbar{}90.35\%\textbar{} \textbar{}7.
ABC\textbar{}5/114\textbar{}95.61\%\textbar{} \textbar{}7.
ABC\textbar{}5/114\textbar{}95.61\%\textbar{} \textbar{}8.
GNB\textbar{}11/114\textbar{}90.35\%\textbar{} \textbar{}8.
GNB\textbar{}6/114\textbar{}92.98\%\textbar{} \textbar{}9.
QDA\textbar{}5/114\textbar{}95.61\%\textbar{} \textbar{}9.
QDA\textbar{}5/114\textbar{}95.61\%\textbar{} \textbar{}10.
SGD\textbar{}19/114\textbar{}83.33\%\textbar{} \textbar{}10.
SGD\textbar{}19/114\textbar{}83.33\%\textbar{} \textbar{}11.
NNC\textbar{}5/114\textbar{}95.61\%\textbar{} \textbar{}11.
NNC\textbar{}7/114\textbar{}93.86\%\textbar{}

\textbf{10 features migliori}

\begin{longtable}[]{@{}lcr@{}}
\toprule
Classificatore & Errori Commessi & Percentuale letta
correttamente\tabularnewline
\midrule
\endhead
1. LogReg & 7/114 & 93.86\%\tabularnewline
2. SVM & 8/114 & 92.98\%\tabularnewline
3. DTC & 7/114 & 93.86\%\tabularnewline
4. KNC & 9/114 & 92.11\%\tabularnewline
5. RFC & 5/114 & 95.61\%\tabularnewline
6. MLP & 10/114 & 91.23\%\tabularnewline
7. ABC & 4/114 & 96.49\%\tabularnewline
8. GNB & 8/114 & 92.98\%\tabularnewline
9. QDA & 7/114 & 93.86\%\tabularnewline
10. SGD & 14/114 & 87.72\%\tabularnewline
11. NNC & 6/114 & 94.74\%\tabularnewline
\bottomrule
\end{longtable}

Osservando le tabelle è possibile notare come i classificatori migliori
risultino: 1. ABC che compare tra le performance migliori in presenza di
9,16,30 e 10 best ma non in 1 feature 2. QDA che compare tra le
performance migliori in presenza di 1,9,16 e 30 ma non in 10 best 3. NNC
che compare tra le performance migliori in presenza di 9,16,10 best ma
non in 1,30 features.

Bisogna specificare che il fatto che questi classificatori non compaiano
tra le performance migliori in alcuni casi, non vuol dire che
classifichino male ma che altri classificatori sono risultati migliori,
a parità di condizioni. Infatti: 1. ABC ha una performance dell'
87.72\%, rispetto a SVM (92.11\% = migliore performance) in presenza di
1 feature 2. QDA ha una performane del 93.86\%, rispetto a ABC (96.49\%
= migliore perfomance) in presenza di 10 best 3. NNC ha una performance:
1. dell' 86.84\% rispetto a SVM (92.11\% = migliore performance) in
presenza di 1 feature 2. del 93.86\% rispetto a SVM (95.61\% = migliore
performance) in presenza di 30 features

Inoltre, in generale si può notare come le performance dei
classificatori migliorino significativamente all'aumentare del numero di
features prese in considerazione durante la classificazione. Questo è in
linea con quanto ci si aspetta teoricamente quando si parla di
"maledizione" della dimensionalità.

Il fatto che i 3 classificatori migliori siano ABC, QDA e NNC è
spiegabile con: 1. ABC e NNC sono classificatori adattativi. Inoltre, in
generale, i classificatori basati su reti neurali sono la soluzione più
rapida e diretta per avere performance consistenti; 2. QDA si basa su
una linea di decisione quadratica e sul teorema di Bayes. I
classificatori bayesiani sono caratterizzati dalla minima probabilità di
errore (Bayes Error Rate), ovvero migliore che un classificatore possa
fare. Il fatto che GNB non lavori bene come QDA potrebbe essere legato
alla minore flessibilità del primo modello in cui si fa anche
un'assunzione di non correlazione tra le variabili all'interno di una
stessa classe.

    \paragraph{Extra}\label{extra}

    In questa sezione, sono mostrati per completezza di contenuti, i
DataFrame con i dati relativi alle accuratezze dei classificatori per i
vari splitting tra training e test. \textbf{Dataframe: 1 feature}
\textbf{Dataframe: 9 features} \textbf{Dataframe: 16 features}
\textbf{Dataframe: 30 features} \textbf{Dataframe: 10 features migliori}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
