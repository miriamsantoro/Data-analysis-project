{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confronto di classificatori per il Dataset WDBC\n",
    "\n",
    "## Elaborato per il corso di Analisi Dati\n",
    "### Miriam Santoro\n",
    "#### aa. 2017/2018, Università di Bologna - Corso di laurea magistrale in Fisica Applicata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scopo del progetto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il presente lavoro è stato realizzato in linguaggio `Python3` e si pone come obiettivo l'implementazione e la valutazione di:\n",
    "1. 10 classificatori creati utilizzando le funzioni della libreria `sklearn`,\n",
    "2. un classificatore basato su una rete neurale costruito tramite la libreria `pytorch`.\n",
    "\n",
    "Il progetto ha previsto anche l'implementazioni di altre librerie, oltre quelle già menzionate, quali:\n",
    "1. `numpy`\n",
    "2. `pandas`\n",
    "3. `matplotlib`\n",
    "4. `scipy`\n",
    "5. `graphviz`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esecuzione del progetto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I classificatori sono stati implementati in due script diversi a seconda della libreria utilizzata. Nello specifico:\n",
    "1. `Classification.py` contiene i classificatori definiti usando `sklearn`; nello specifico:\n",
    "    1. 10 classificatori implementati con possibilità di scegliere le features;\n",
    "    2. Gli stessi 10 classificatori implementati tenendo conto delle 10 features migliori.\n",
    "2. `NNC.py` contiene il classificatore definito usando Pytorch; nello specifico:\n",
    "    1. Il classificatore implementato con possibilità di scegliere le features\n",
    "    2. Lo stesso classificatore implementato tenendo conto delle 10 features migliori \n",
    "\n",
    "\n",
    "\n",
    "Questi due file Python sono stati implementati in `main.py`, script che è necessario eseguire per ottenere i risultati.\n",
    "Inoltre, in `main.py` è stato importato anche lo script `Plotting.py`, utile per visualizzare i vari plot che verranno descritti nell'apposita sezione **Plotting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset (note preliminari)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il WDBC *Wisconsis Diagnostic Breast Cancer* è un dataset contenente 569 istanze, corrispondenti a 569 pazienti, ognuno dei quali classificato tramite un Id-paziente, un lettera indicante il tipo di tumore (maligno o benigno) e 30 features.\n",
    "In particolare, le features legate al tumore indicano, in ordine:\n",
    "1. Raggio (media delle distanze dal centro ai punti sul perimetro)\n",
    "2. Texture (deviazione standard dei valori in scala di grigi)\n",
    "3. Perimetro \n",
    "4. Area\n",
    "5. Smoothness (variazione locale nelle lunghezze del raggio)\n",
    "6. Compattezza (perimetro^2/area -1.0)\n",
    "7. Concavità (gravità delle porzioni concave del contorno)\n",
    "8. Punti concavi (numeri di porzioni concave del contorno)\n",
    "9. Simmetria\n",
    "10. Dimensioni frattali (\"approssimazione coastline\" -1)\n",
    "\n",
    "Le features sono state calcolate da un'immagine digitalizzata di un fine aspiratore (FNA) di una massa del seno e descrivono le caratteristiche dei nuclei cellulari nell'immagine. Nello specifico per ogni immagine si hanno 30 features corrispondenti alla media, all'errore stardard e alla \"peggiore\" o più grande di ogni misura nell'elenco puntato e ogni cifra è stata acquisita con 4 digits. \n",
    "\n",
    "Inoltre, è necessario aggiungere che le 569 istanze possono essere divise in due classi, la prima avente 357 tumori benigni e la seconda avente 212 tumori maligni.\n",
    "\n",
    "\n",
    "Una volta estratti i dati tramite la libreria `pandas` e aver disposto features e labels in array si è utilizzata la funzione `model_selection.train_test_split` contenuta in `sklearn` per splittare in maniera diversa il dataset in training set e test set. Questi ultimi sono stati divisi come segue:\n",
    "1. 90% training, 10% test\n",
    "2. 80% training, 20% test\n",
    "3. 50% training, 50% test \n",
    "4. 25% training, 75% test\n",
    "\n",
    "Le divisioni sono state pensate in modo da evitare l'overfitting e valutare in varie condizioni le performance di tutti i classificatori utilizzati.\n",
    "Le diverse divisioni sono state implementate tramite un ciclo for all'interno di tutti i classificatori, come è mostrato nelle seguenti righe di codice:\n",
    "\n",
    "```python\n",
    "seq = [.9, .8, .5, .25]\n",
    "for i in seq:\n",
    "    X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, train_size=i, test_size= 1-i, \n",
    "                                                                        random_state=0)\n",
    "```   \n",
    "\n",
    "Per quanto riguarda il classificatore implementato tramite la libreria `pytorch` è stato necessario sfuttare il one hot encoding, un processo tramite cui è possibile convertire le labels in numeri binari. In questo caso, quindi, sono stati associati $[1,0]$ e $[0,1]$ alle due possibili uscite (rispettivamente $B$ e $M$) e questo passaggio è risultato indispensabile in quanto `pytorch` è in grado di funzionare solo su dati numerici."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisi dei classificatori utilizzati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si analizzano e commentano di seguito i classificatori utilizzati in questo progetto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificatori da `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I classificatori implementati grazie `sklearn` sono i seguenti:\n",
    "1. LogReg (*Logistic Regression classifier*)\n",
    "2. SVM (*Support Vector Machine classifier*)\n",
    "3. DTC (*Decision Tree Classifier*)\n",
    "4. KNC (*K Neighbor Classifier*)\n",
    "5. RFC (*Random Forest Classifier*)\n",
    "6. MLP (*Multi Layer Perceptron classifier*)\n",
    "7. ABC (*Ada Boost Classifier*)\n",
    "8. GNB (*Gaussian Naive Bayes classifier*)\n",
    "9. QDA (*Quadratic Discriminant Analysis classifier*)\n",
    "10. SGD (*Stochastic Gradient Descent classifier*)\n",
    "\n",
    "In un primo momento, ciascuno di questi classificatori è stato eseguito e valutato al variare del numero di features. Nello specifico sono state prese in considerazione le seguenti casistiche:\n",
    "1. 1 features \n",
    "2. 9 features\n",
    "3. 16 features\n",
    "4. 30 features\n",
    "\n",
    "Per fare ciò è stato solo necessario cambiare i parametri d'ingresso alle funzioni utilizzate per ciascun classificatore, come è possibile notare nel seguente esempio di codice:\n",
    "```python\n",
    "#da main.py\n",
    "l1 = Classification.LogReg(2,3)\n",
    "m1 = Classification.LogReg(2,9)\n",
    "n1 = Classification.LogReg(2,16)\n",
    "o1 = Classification.LogReg(2,30)\n",
    "```\n",
    "\n",
    "Inoltre, dopo che, tramite la funzione `stats.spearmanr`, implementata all'interno di `Histo` in `Plotting.py`, sono state individuate le 10 features più correlate alle labels, sono stati realizzati classificatori uguali a quelli precedenti ma in modo che prendessero come input queste 10 specifiche features. \n",
    "Per fare ciò, si è aggiunto un 10 al nome delle funzioni precedenti usate per i classificatori e non sono stati forniti parametri di input nell'argomento della funzione.\n",
    "La chiamata a uno di questi classificatori è mostrata nella seguente riga di codice, tratta dal main:\n",
    "```python\n",
    "p1 = Classification.LogReg10()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. LogReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si usa l'implementazione standard del classificatore Logistic Regression contenuto nella libreria `sklearn.linear_model`. Questo è usato per valutare i parametri di un modello statistico al fine di modellare una variabile dipendente binaria, ovvero una variabile con due possibili valori (nel nostro caso etichettati con \"B\"=0 e \"M\"=1).\n",
    "La funzione utilizzata in questo modello è chiamata logistica ed è una funzione del tipo:\n",
    "\n",
    "$\\begin{equation}\n",
    "f(x)={\\frac {L}{1+e^{-k(x-x_{0})}}}\\\\\n",
    "\\end{equation}$\n",
    "\n",
    "dove:\n",
    "- e = numero di Eulero,\n",
    "- x0 = valore sull'asse delle x del punto a metà della funzione sigmoide,\n",
    "- L =  massimo valore della curva\n",
    "- k = ripidezza della curva.\n",
    "\n",
    "Nello specifico, per quanto riguarda il classificatore usato in questo progetto e mostrato di seguito:\n",
    "```python\n",
    "#da Classification.py\n",
    "cl = linear_model.LogisticRegression(C=2.5)\n",
    "```\n",
    "si sono usati:\n",
    "- $C=2.5$ come inverso della forza di regolarizzazione, indicante la tolleranza di casi mal classificati, in quanto questo valore è poco al di sopra del valore di default 1 ed è utile per evitare errori di overfitting;\n",
    "- altri parametri lasciati di default tra cui:\n",
    "    - $\\text{solver='liblinear'}$ come risolutore, in quanto per piccoli dataset è una buona scelta\n",
    "    - $\\text{penalty='l2'}$ come termine di penalizzazione\n",
    "    \n",
    "In seguito all'addestramento del classificatore tramite la funzione `fit`, viene utilizzata la funzione `predict` per fare previsioni sui dati di test e vengono calcolati rispettivamente report di classificazione, matrice di confusione e accuratezza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si usa l'implementazione standard della Support Vector Classification contenuta nella libreria `sklearn.svm`. Questo classificatore è detto anche classificatore a massimo margine perchè allo stesso tempo minimizza l'errore empirico di classificazione e massimizza il margine geometrico. Nello specifico, cerca gli iperpiani di separazioni ottimali tramite una funzione obiettivo senza minimi locali.\n",
    "\n",
    "Tra i vantaggi nell'uso del SVC possiamo trovare il fatto che:\n",
    "- sia efficiente dal punto di vista della memoria in quanto usa un subset di punti di training nella funzione di decisione (chiamati support vectors) \n",
    "- sia versatile poichè per come funzioni di decisione si possono specificare diverse funzioni Kernel. Queste permettono di proiettare il problema iniziale su uno spazio di dimensioni superiore senza grandi costi computazionali e ottenendo separazioni basate anche su superfici non lineari.\n",
    "\n",
    "Nello specifico, per quanto riguarda il classificatore usato in questo progetto e mostrato di seguito:\n",
    "```python\n",
    "#da Classification.py\n",
    "cl=svm.SVC(kernel='linear')    \n",
    "```\n",
    "si sono usati:\n",
    "- come funzione kernel, una funzione lineare del tipo:\n",
    "    $\\begin{equation*}\n",
    "    \\langle x, x'\\rangle\n",
    "    \\end{equation*}$\n",
    "- altri parametri di default, tra cui:\n",
    "   - $C=1.0$ come parametro di penalità per il termine di errore\n",
    "\n",
    "In seguito all'addestramento del classificatore tramite la funzione `fit`, viene utilizzata la funzione `predict` per fare previsioni sui dati di test e vengono calcolati rispettivamente report di classificazione, matrice di confusione e accuratezza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. DTC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si usa l'implementazione standard del Decision Tree Classifier contenuto nella libreria `sklearn.tree`. Questo classificatore è usato per valutare i parametri di un modello al fine di predire il valore di un target variabile, apprendendo semplici regole di decisione dedotte dalle features.\n",
    "\n",
    "Tra i vantaggi di questo classificatore è possible trovare il fatto che:\n",
    "- sia semplice da capire ed interpretare in quanto si basa su un modello white box in cui l'interpretazione di una condizione è facilmente spiegata dalla logica booleana\n",
    "- gli alberi (*trees*) possano essere visualizzati\n",
    "- il costo di predire i dati usando l'albero sia logaritmico nel numero di punti di dati usati per allenare l'albero stesso.\n",
    "\n",
    "\n",
    "Nello specifico, per quanto riguarda il classificatore implementato in questo progetto e mostrato di seguito:\n",
    "```python\n",
    "#da Classification.py\n",
    "cl = tree.DecisionTreeClassifier()      \n",
    "```\n",
    "si sono usati i parametri di default, tra cui:\n",
    "- $\\text{criterion='gini'}$ come funzione per misurare la qualità dello split; \n",
    "- $\\text{splitter='best'}$ come strategia usata per scegliere lo split ad ogni nodo in modo da scegliere quello migliore;\n",
    "- $\\text{max_features=None}$ come numero di features da considerare quando si guarda allo split migliore. In questo caso max_features=n_features;\n",
    "- $\\text{max_depth=None}$ come massima profondità dell'albero None. Questo significa che i nodi vengono estesi fin quando tutte le foglie sono pure.\n",
    "\n",
    "Inoltre, l'equazione di diminuzione di impurità pesata (che governa lo split) è:\n",
    "\n",
    "$\\begin{equation}\n",
    "\\frac{N_t}{N} \\cdot \\left(\\text{impurity} - \\frac{N_{t_R}}{N_t} \\cdot \\text{right_impurity} - \\frac{N_{t_L}}{N_t} \\cdot \\text{left_impurity} \\right) \\\\\n",
    "\\end{equation}$\n",
    "\n",
    "dove:\n",
    "- $N$ è il numero totale di campioni\n",
    "- $N_t$ è il numero di campioni al nodo corrente\n",
    "- $N_{t_L}$ è il numero di campioni nella parte destra\n",
    "- $N_{t_R}$ è il numero di campioni nella parte sinistra\n",
    "\n",
    "In seguito all'addestramento del classificatore tramite la funzione `fit`, viene utilizzata la funzione `predict` per fare previsioni sui dati di test e vengono calcolati rispettivamente report di classificazione, matrice di confusione e accuratezza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. KNC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si usa l'implementazione standard del K Neighbors Classifier contenuto nella libreria `sklearn.neighbors`. Questo classificatore non cerca di costruire un modello interno generale, ma semplicemente memorizza le istanze dei dati di training; quindi la classificazione è calcolata da un semplice voto di maggioranza dei vicini più vicini ad ogni punto: il punto di ricerca è assegnato alla classe che ha il maggior numero di rappresentanti nei vicini più vicini del punto.\n",
    "\n",
    "Questo classificatore, quindi, si basa su k vicini, dove k è un valore intero specificato dall'utente e la sua scelta ottimale dipende fortemente dai dati. Ad esempio, in generale, una k più grande sopprime gli effetti del rumore ma rende i confini di classificazione meno distinti.\n",
    "\n",
    "Nello specifico, per quanto riguarda il classificatore implementato in questo progetto e mostrato di seguito:\n",
    "```python\n",
    "#da Classification.py\n",
    "cl = neighbors.KNeighborsClassifier(n_neighbors=3)       \n",
    "```\n",
    "si sono usati:\n",
    "- $\\text{n_neighbors=3}$ come numero di vicini;\n",
    "- altri parametri di default tra cui:\n",
    "    - $\\text{weights='uniform'}$ come funzione per i pesi usata per la previsione. I pesi uniformi portano a pesare equamente tutti i punti in ogni vicinato\n",
    "    - $\\text{algorithm='auto'}$ come algoritmo usato per calcolare i vicini. Questo è l'algoritmo più appropriato sulla base dei valori passati dal metodo di fit.\n",
    "    - $\\text{metric='minkowski'}$ come metrica ovvero distanza usata per l'albero.\n",
    "    \n",
    "In seguito all'addestramento del classificatore tramite la funzione `fit`, viene utilizzata la funzione `predict` per fare previsioni sui dati di test e vengono calcolati rispettivamente report di classificazione, matrice di confusione e accuratezza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. RFC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si usa l'implementazione standard del Random Forest Classifier contenuto nella libreria `sklearn.ensemble`. Questo classificare fa il fit di un numero di classificatori decision trees su vari sotto-campioni del dataset e usa la media per migliorare l'accuratezza predittiva e controllare l'over-fitting. La grandezza dei sotto campioni è uguale a quella del campione di input iniziale ma i campioni sono disegnati con rimpiazzamento dal set di training. \n",
    "Inoltre, quando si fa la divisione di un nodo durante la costruzione dell'albero, lo split che viene scelto non è il migliore tra tutte le features ma tra un subset random di features. A causa di questa randomicità, il bias della foresta dovrebbe aumentare (rispetto a quello di un singolo albero non-random) ma, grazie alla media, la sua varianza diminuisce e compensa di più l'aumento del bias determinando un modello migliore.\n",
    "\n",
    "Nello specifico, per quanto riguarda il classificatore usato in questo progetto e mostrato di seguito:\n",
    "```python\n",
    "#da Classification.py\n",
    "cl=ensemble.RandomForestClassifier(max_depth=15, n_estimators=10, max_features=1)      \n",
    "```\n",
    "si sono usati:\n",
    "- $\\text{max_depth=15}$ come massima espansione dell'albero;\n",
    "- $\\text{n_estimators=10}$ come numero di alberi (estimatori) nella foresta;\n",
    "- $\\text{max_features=1}$ come numero di features da considerare quando si guarda al migliore split;\n",
    "- altri parametri di default, tra cui:\n",
    "    - $\\text{criterion='gini'}$ come funzione per misurare la qualità dello split;\n",
    "    - $\\text{min_samples_split=2}$ come numero minimo di campioni richiesto per dividere un nodo interno.\n",
    "\n",
    "In seguito all'addestramento del classificatore tramite la funzione `fit`, viene utilizzata la funzione `predict` per fare previsioni sui dati di test e vengono calcolati rispettivamente report di classificazione, matrice di confusione e accuratezza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si usa l'implementazione standard del Multi Layer Perceptron contenuto nella libreria `sklearn.neural_network`. Questo classificatore si basa su una rete neurale e su un allenamento iterativo. Infatti, ad ogni step temporale vengono calcolate le derivate parziali della funzione di perdita rispetto ai parametri del modello per aggiornare i parametri stessi.\n",
    "Può avere anche un termine di regolarizzazione aggiunto alla funzione di perdita che restringe i parametri del modello per prevenire l'overfitting.\n",
    "\n",
    "Nello specifico, per quanto riguarda il classificatore utilizzato in questo progetto e mostrato di seguito:\n",
    "```python\n",
    "#da Classification.py\n",
    "cl = neural_network.MLPClassifier(activation='logistic', solver='lbfgs', max_iter=1000 )       \n",
    "```\n",
    "si sono usati:\n",
    "- $\\text{activation='logistic'}$ come funzione di attivazione per gli strati nascosti. Questa è una funzione logistica sigmoidale che restituisce:\n",
    "\n",
    "    $\\begin{equation}\n",
    "    f(x) = \\frac{1}{1+exp(-x)}\\\\\n",
    "    \\end{equation}$\n",
    "    \n",
    "- $\\text{solver='lbfgs'}$ come risolutore per l'ottimizzazione dei pesi. Questo è un ottimizzatore facente parte dei metodi quasi-newtoniani ed è stato scelto in quanto per piccoli datasets, come il wdbc, converge più velocemente e ha migliori performance;\n",
    "- $\\text{max_iter=1000}$ come massimo numero di iterazioni per la convergenza;\n",
    "- altri parametri di default, tra cui:\n",
    "    - $\\text{hidden_layer_sizes=(100,)}$ come dimensioni degli strati nascosti;\n",
    "    - $\\text{alpha=0.0001}$ come parametro di penalizzazione L2;\n",
    "    - $\\text{batch_size=min(200,n_samples)}$ come grandezza di batch;\n",
    "    - $\\text{learning_rate=costant=0.001}$ come frequenza di apprendimento per l'aggiornamento dei pesi.\n",
    " \n",
    "In seguito all'addestramento del classificatore tramite la funzione `fit`, viene utilizzata la funzione `predict` per fare previsioni sui dati di test e vengono calcolati rispettivamente report di classificazione, matrice di confusione e accuratezza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. ABC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si usa l'implementazione standard dell'Ada Boost Classifier contenuto nella libreria `sklearn.ensemble`. Questo classificatore inizialmente fa il fit di un modello debole (leggermente migliori di quelli random) sul dataset e poi fitta copie aggiuntive dello stesso modello sullo stesso dataset ma aggiustando i pesi di istanze di training classificate non correttamente in modo che i classificatori seguenti si focalizzino di più su casi difficili. \n",
    "\n",
    "Iniziamente, questi pesi sono tutti settati a 1/N, così che il primo step alleni semplicemente un debole modello dei dati originali; tuttavia, per ogni iterazione successiva, sono modificati individualmente in modo che:\n",
    "- i pesi associati agli esempi di training che non sono predetti correttamente vengano aumentati;\n",
    "- i pesi associati agli esempi di training che sono predetti correttamente vengano diminuiti.\n",
    "\n",
    "Successivamente l'algoritmo di apprendimento viene riapplicato per ripesare i dati. In questo modo ogni volta che le iterazioni procedono, gli esempi difficili da predire ricevono un'influenza sempre crescente e ogni modello debole viene forzato a concentrarsi sugli esempi che mancano dai precedenti nella sequenza.\n",
    "Infine, le previsioni da ciascuno di loro vengono combinate attraverso un voto di maggioranza pesata per produrre la previsione finale. \n",
    "\n",
    "Nello specifico, per quanto riguarda il classificatore usato in questo progetto e mostrato di seguito:\n",
    "```python\n",
    "#da Classification.py\n",
    "cl=ensemble.AdaBoostClassifier()       \n",
    "```\n",
    "si sono usati i parametri di default, tra cui:\n",
    "- $\\text{base_estimator=None=DecisionTreeClassifier(max_depth=1)}$ come estimatore base da cui è costruito l'ensemble potenziato;\n",
    "- $\\text{n_estimators=50}$ come numero di estimatori a cui viene concluso il potenziamento;\n",
    "- $\\text{learning_rate=1}$ come frequenza di apprendimento;\n",
    "- $\\text{algorithm='SAMME.R'}$ come algoritmo di potenziamento. Questo converge velocemente, raggiungendo un errore minore di testing con minori iterazioni di boosting.\n",
    "\n",
    "In seguito all'addestramento del classificatore tramite la funzione `fit`, viene utilizzata la funzione `predict` per fare previsioni sui dati di test e vengono calcolati rispettivamente report di classificazione, matrice di confusione e accuratezza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. GNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si usa l'implementazione standard del Gaussian Naive Bayes contenuto nella libreria `sklearn.naive_bayes`. Questo classificatore si basa sull'applicazione del teorema di Bayes, con l'assunzione di una forte (naive) indipendenza condizionata tra ogni coppia di features, dato il valore della variabile di classe.\n",
    "Il teorema di Bayes stabilisce che, data la variabile di classe $y$ e il vettore di feature dipendente $x_1$ attraverso $x_n$, si ha:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots x_n \\mid y)} {P(x_1, \\dots, x_n)} \\\\\n",
    "\\end{equation}$\n",
    "\n",
    "L'assunzione di indipendenza naive condizionale, invece, stabilisce che:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(x_i | y, x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n) = P(x_i | y) \\\\\n",
    "\\end{equation}$\n",
    "\n",
    "Questo classificatore implementa l'algoritmo Gaussian Naive Bayes per la classificazione in cui si assume che la likelihood delle features sia gaussiana:\n",
    "$\\begin{equation}\n",
    "P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right) \\\\\n",
    "\\end{equation}$\n",
    "\n",
    "dove i parametri $\\sigma_y$ e $\\mu_y$ sono stimati usando il massimo della likelihood.\n",
    "\n",
    "Tra i vantaggi di questo classificatore troviamo il fatto che:\n",
    "- richiede una piccola quantità di dati di training per stimare i parametri necessari;\n",
    "- può essere estremamente veloce se confrontato con metodi più sofisticati;\n",
    "- il disaccoppiamento delle distribuzioni di features condizionali delle classi può essere stimato indipendentemente come una distribuzione unidimensionale e questo aiuta ad ridurre i problemi che derivano dalla 'maledizione' della dimensionalità.\n",
    "\n",
    "\n",
    "Nello specifico, per quanto riguarda il classificatore usato in questo progetto e mostrato di seguito:\n",
    "```python\n",
    "#da Classification.py\n",
    "cl=naive_bayes.GaussianNB()    \n",
    "```\n",
    "in cui si sono utilizzati i parametri di default, ovvero:\n",
    "- $\\text{priors=(n_classes,)}$ come probabilità a priori delle classi;\n",
    "- $\\text{var_smoothing=1e-9}$ come porzione della varianza più grande tra tutte le features. Questa viene aggiunta alle varianze per il calcolo della stabilità.\n",
    "\n",
    "In seguito all'addestramento del classificatore tramite la funzione `fit`, viene utilizzata la funzione `predict` per fare previsioni sui dati di test e vengono calcolati rispettivamente report di classificazione, matrice di confusione e accuratezza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. QDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si usa l'implementazione standard della Quadratic Discriminant Analysis contenuta nella libreria `sklearn.discriminant_analysis`. Questo classificatore può essere ottenuto da semplici modelli probabilistici che modellano la distribuzione condizionale di classe dei dati $P(X|y=k)$ per ogni classe $k$. \n",
    "Le previsioni possono essere ottenute usando la regola di Bayes:\n",
    "\n",
    "$\\begin{equation*}\n",
    "P(y=k | X) = \\frac{P(X | y=k) P(y=k)}{P(X)} \\\\\n",
    "\\end{equation*}$\n",
    "\n",
    "Nello specifico, $P(X|y)$ è modellizzato come una distribuzione Gaussiana multivariata con densità:\n",
    "\n",
    "$\\begin{equation*}\n",
    "P(X | y=k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}}\\exp\\left(-\\frac{1}{2} (X-\\mu_k)^t \\Sigma_k^{-1} (X-\\mu_k)\\right) \\\\\n",
    "\\end{equation*}$\n",
    "\n",
    "dove $d$ è il numero di features.\n",
    "\n",
    "In poche parole, questo è un classificatore con un limite di decisione (superficie di separazione) quadratico che è generato fittando le densità condizionali delle classi e usando la regola di Bayes. Il modello fitta una densità Gaussiana ad ogni classe.\n",
    "\n",
    "Nello specifico, per quanto riguarda il classificatore usato in questo progetto e mostrato di seguito:\n",
    "```python\n",
    "#da Classification.py\n",
    "cl=discriminant_analysis.QuadraticDiscriminantAnalysis()      \n",
    "```\n",
    "sono stati usati come parametri quelli di default, tra cui:\n",
    "- $\\text{priors=n_classes}$ come priori sulle classi;\n",
    "- $\\text{tol=1.0e-4}$ come soglia usata per la stima del rango.\n",
    "\n",
    "In seguito all'addestramento del classificatore tramite la funzione `fit`, viene utilizzata la funzione `predict` per fare previsioni sui dati di test e vengono calcolati rispettivamente report di classificazione, matrice di confusione e accuratezza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si usa l'implementazione standard della Stochastic Gradient Descent contenuta nella libreria `sklearn.linear_model`. Questo è un classificatore lineare con apprendimento tramite il gradiente di discesa stocastica (SGD); nello specifico, questo implica che per ogni campione:\n",
    "- viene stimato il gradiente di perdita;\n",
    "- viene aggiornato il modello man mano con una frequenza di apprendimento decrescente.\n",
    "\n",
    "Il regolarizzatore è un termine di penalità che viene aggiunto alla funzione di perdita e che restringe i parametri del modello.\n",
    "\n",
    "Nello specifico, per quanto riguarda il classificatore usato in questo progetto e mostrato di seguito:\n",
    "```python\n",
    "#da Classification.py\n",
    "cl = linear_model.SGDClassifier(loss=\"perceptron\", penalty=\"elasticnet\", max_iter=600)   \n",
    "```\n",
    "si sono usati:\n",
    "- $\\text{loss='perceptron'}$ come funzione di perdita, ovvero come perdita lineare usata dall'algoritmo del percettrone;\n",
    "- $\\text{penalty='elasticnet'}$ come penalità (termine di regolarizzazione). Questo è un termine che viene aggiunto alla funzione di perdita, restringe i parametri del modello e, in questo caso, è una combinazione di L2 (norma euclidea quadrata) e L1 (norma assoluta);\n",
    "- $\\text{max_iter=600}$ come massimo numero di passi sui dati di training;\n",
    "- altri parametri di default, tra cui:\n",
    "    - $\\text{tol=1e-3}$ come criterio di stop.\n",
    "\n",
    "In seguito all'addestramento del classificatore tramite la funzione `fit`, viene utilizzata la funzione `predict` per fare previsioni sui dati di test e vengono calcolati rispettivamente report di classificazione, matrice di confusione e accuratezza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificatore da `pytorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NNC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per l'implementazione della CNN è stata utilizzata la libreria `pytorch` che sfrutta una sintassi basata su tensori e programmazione ad oggetti.\n",
    "\n",
    "Come struttura della rete neurale, si sono utilizzati: \n",
    "1. un livello lineare che va da e-s a 15 canali, dove s ed e sono rispettivamente la prima e l'ultima feature considerate;\n",
    "2. un livello di passaggio per la funzione ReLU, utile per scartare i valori negativi ottenuti finora;\n",
    "3. un livello lineare che va da 15 a 10 canali;\n",
    "4. un livello di passaggio per la funzione ReLU, avente lo stesso scopo del livello 2;\n",
    "5. un livello finale lineare che va da 10 a 2 canali.\n",
    "\n",
    "Per l'addestramento della CNN si sottopone alla rete neurale l'intero dataset mescolato più volte, dove il numero di \"mescolamenti\" è detto `epoch`.\n",
    "\n",
    "Come funzione di errore si è scelta `SmoothL1Loss` perchè ritenuta adeguata per una classificazione 2-classi e perchè ritenuta migliore rispetto alle altre funzioni di errore candidate. Nello specifico, questa crea un criterio che usa un termine quadrato se l'errore assoluto element-wise cade al di sotto dell'1 e un termine L1 altrimenti. Inoltre, è meno sensibile ai valori anomali rispetto al MSELoss ed è conosciuta anche come Huber loss:\n",
    "\n",
    "$\\begin{equation}\n",
    "\\text{loss}(x, y) = \\frac{1}{n} \\sum_{i} z_{i} \\\\\n",
    "\\end{equation}$\n",
    "\n",
    "dove $z_i$ è data da:\n",
    "\n",
    "$\\begin{equation}\n",
    "\\begin{split}z_{i} =\n",
    "\\begin{cases}\n",
    "0.5 (x_i - y_i)^2, & \\text{se } |x_i - y_i| < 1 \\\\\n",
    "|x_i - y_i| - 0.5, & \\text{altrimenti }\n",
    "\\end{cases}\\end{split}\n",
    "\\end{equation}$\n",
    "\n",
    "Insieme a questa è stata usata come funzione di ottimizzazione `torch.optim.Adam`, una funzione adattiva dotata di gradient descent e suggerita dalla documentazione di `pytorch` per l'addestramento.\n",
    "Durante l'addestramento si calcola quindi la backpropagation dell'errore lungo tutta la rete e si esegue uno step di ottimizzazione la cui dimensione è definita dal `learning_rate`(=0.005, in questo caso) nella funzione `torch.optim.Adam`.\n",
    "\n",
    "Per quanto riguarda il testing e l'accuratezza delle previsioni fatte dalla rete neurale si sono usate le funzioni `np.argmax` contenute nella libreria `sklearn`.\n",
    "\n",
    "Di seguito, si riporta lo script della rete neurale, comprendente addestramento e testing:\n",
    "```python\n",
    "#da NNC.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "def NNC(s,e):\n",
    "    myList = []\n",
    "    f=open(\"wdbc.data.txt\")\n",
    "    data_train = pd.read_csv(\"wdbc.data.txt\", header=None, sep=r\"\\s+\")\n",
    "    X = np.array(data_train)\n",
    "    \n",
    "    pre_lab=X[:,1]  \n",
    "    features=X[:,s:e]\n",
    "    \n",
    "    pre_labels=[]\n",
    "    label_encoder = LabelEncoder()                                  \n",
    "    pre_labels = label_encoder.fit_transform(pre_lab)\n",
    "    pre_labels = np.array(pre_labels)\n",
    "    \n",
    "    labels=[]\n",
    "    for num in range(len(pre_labels)):\n",
    "        if pre_labels[num] == 0:\n",
    "            labels.append([1,0])\n",
    "        if pre_labels[num] == 1:\n",
    "            labels.append([0,1])\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    seq=[.9, .8, .5, .25]\n",
    "    for i in seq:\n",
    "\n",
    "        features_train, features_test, labels_train, labels_test = model_selection.train_test_split(features, \n",
    "                                                                                                    labels, train_size=i,\n",
    "                                                                                                    test_size=1-i,\n",
    "                                                                                                    random_state=0)\n",
    "        features_train = np.array(features_train, dtype=np.float32)\n",
    "        features_test = np.array(features_test, dtype=np.float32)\n",
    "        labels_train = np.array(labels_train, dtype=np.int64)\n",
    "        labels_test = np.array(labels_test, dtype=np.int64)\n",
    "        \n",
    "        features_train_v = Variable(torch.Tensor(features_train), requires_grad = False)\n",
    "        labels_train_v = Variable(torch.Tensor(labels_train), requires_grad = False)\n",
    "        features_test_v = Variable(torch.Tensor(features_test), requires_grad = False)\n",
    "        labels_test_v = Variable(torch.Tensor(labels_test), requires_grad = False)\n",
    "         \n",
    "        model = nn.Sequential(nn.Linear(e-s, 15),    \n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(15, 10),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(10,2))\n",
    "        \n",
    "        loss_fn = torch.nn.SmoothL1Loss()\n",
    "        optim = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "        \n",
    "        #training\n",
    "        all_losses=[]\n",
    "        for num in range(2000):\n",
    "            optim.zero_grad()                                 # Intialize the hidden weights to all zeros\n",
    "            pred = model(features_train_v)                    # Forward pass: Compute the output class\n",
    "            loss = loss_fn(pred, labels_train_v)              # Compute the loss: difference between the output class and                                                                      the pre-given label\n",
    "            all_losses.append(loss.data)\n",
    "            loss.backward()                                   # Backward pass: compute the weight\n",
    "            optim.step()                                      # Optimizer: update the weights of hidden nodes\n",
    "            \n",
    "            print('epoch: ', num,' loss: ', loss.item())      # Print statistics       \n",
    "            \n",
    "        #testing\n",
    "        predicted_values=[]\n",
    "    \n",
    "        for num in range(len(features_test_v)):\n",
    "            predicted_values.append(model(features_test_v[num]))\n",
    "        \n",
    "        score = 0\n",
    "        for num in range(len(predicted_values)):\n",
    "            if np.argmax(labels_test[num]) == np.argmax(predicted_values[num].data.numpy()):\n",
    "                score = score + 1     \n",
    "        accuracy = float(score / len(predicted_values)) * 100\n",
    "        \n",
    "        if accuracy <= 70:\n",
    "            print (\"Testing Accuracy Score is: %0.2f \" % (accuracy))\n",
    "            print(\"Run it again!\")\n",
    "        else:\n",
    "            print (\"Testing Accuracy Score is: %0.2f \" % (accuracy))\n",
    "    \n",
    "        myList.append(accuracy)\n",
    "    \n",
    "    print(myList)    \n",
    "    return myList \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le seguenti funzioni sono implementate in `Plotting.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione plot è stata implementata tramite la libreria `matplolib`. In questo modo è possibile visualizzare in un plot bidimensionale la popolazione del dataset iniziale e dei vari splitting effettuati per ottenere training set e test set.\n",
    "I dati con cui è stato riempito il plot bidimensionale sono solo legati all'id-paziente e al tipo di tumore, non alle features.\n",
    "E' stato possibile realizzare questi grafici tramite la funzione `scatter`. \n",
    "I risultati sono mostrati nelle seguenti figure:\n",
    "<img src=\"files/img/Plot_1.png\">\n",
    "<img src=\"files/img/Plot_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione è stata implementata tramite la libreria `matplotlib` ed è stata creata per visualizzare graficamente, tramite un istogramma, i risultati riguardanti la correlazione tra le varie features e le labels del dataset in modo da decretare le 10 features migliori. \n",
    "La correlazione, non avendo dati distribuiti normalmente, è stata studiata tramite il coefficiente di Spearman. Infatti, è stata ottenuta tramite la funzione `spearmanr` implementata dalla libreria `scipy.stats` e restituente valori di correlazione compresi tra -1 e 1, dove lo 0 indica la non-correlazione. Come è possibile notare dalla seguente figura, le 10 features classificate come migliori sono quelle corrispondenti alle colonne: 2,4,5,8,9,15,22,24,25,29 del file *wdbc.data.txt* (si ricorda che le prime due colonne sono quelle relative a id-paziente e tipologia del tumore).\n",
    "\n",
    "<img src=\"files/img/Histo.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC plotting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tramite la funzione `SVCPlot(s,e)`, nel caso particolare in cui si abbiano due features di input, vengono plottati quattro diversi SVC e la loro rispettiva decision function. I classificatori usati sono i seguenti:\n",
    "```python\n",
    "#da Plotting.py\n",
    "C=1.0  #parametro di penalità per il termine di errore\n",
    "svc = svm.SVC(kernel='linear', C=C)\n",
    "rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C)\n",
    "rbf_NuSVC = svm.NuSVC(kernel='rbf')\n",
    "lin_svc = svm.LinearSVC(C=C)\n",
    "```\n",
    "dove:\n",
    "1. svc è il classificatore di cui si è parlato nella sezione **SVM**;\n",
    "2. rbf_svc è un classificatore avente come kernel una funzione a base radiale (radial basis function), ovvero del tipo:\n",
    "    \\begin{equation*}\n",
    "    \\exp(-\\gamma \\|x-x'\\|^2)\n",
    "    \\end{equation*}\n",
    "    con $\\gamma$ = 0.7\n",
    "3. rbf_NuSVC è un classificatore simile a SVC ma accetta set di parametri leggermente diversi e ha una diversa formulazione matematica. Nello specifico sfrutta un parametro che controlla il numero di support vectors (limite inferiore) e gli errori di allenamento (limite superiore). Anche in questo caso si è utilizzato come kernel una funzione a base radiale (rbf);\n",
    "4. lin_svc è un classificatore lineare support vector ed è simile al primo classificatore (svc avente un kernel lineare) ma è implementato tramite la libreria `liblinear`, anzichè `libsvm`. Questo permette di avere più flessibilità nella scelta dei termini di penalizzazione e delle funzioni di perdita. I parametri sono lasciati di default e tra questi è possibile notare:\n",
    "    - $\\text{penalty=l2}$ come termine di penalizzazione;\n",
    "    - $\\text{loss='hinge'}$ come funzione di perdita.\n",
    "    \n",
    "Nel caso dell'uso delle due input features mostrate nella seguente riga di codice:\n",
    "```python\n",
    "#da main.py\n",
    "Classification.SVC(2,4)    \n",
    "```\n",
    "il risultato che si ottiene è il seguente:\n",
    "\n",
    "<img src=\"files/img/SVC.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DTC plotting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tramite la funzione `DTCPlot(s:e)` è possibile ottenere in formato pdf il DTC generato, al variare dei diversi splitting di training e test e del numero di features prese in considerazione.\n",
    "In figura è mostrato un esempio di Tree generato, prendendo in considerazione 30 features, un training set del 80% e un test set del 20%.\n",
    "\n",
    "<img src=\"files/img/wdbc_tree_2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot3B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tramite la funzione `Plot3B()` è possibile ottenere un grafico 3-dim avente sui 3 assi le 3 features migliori, dedotte tramite l'istogramma (`Histo()`). Come è possibile notare dalla figura, sembra esserci un andamento lineare sia per i tumori benigni che per quelli maligni ma nel secondo caso, questo andamento presenta delle maggiori variazioni. \n",
    "Il risultato è mostrato di seguito:\n",
    "<img src=\"files/img/3_best_features.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esecuzione del programma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo script finale `main.py` fa uso di tutti i classificatori (`Classification.py`, `NNC.py`) e dello script riguardante i vari plotting (`Plotting.py`). Nello specifico:\n",
    "1. Fa il plot della popolazione iniziale e dei vari splitting implementati, come già spiegato nella sezione **Plot**, tramite la riga di codice:\n",
    "    ```python\n",
    "        Plotting.Plot()\n",
    "    ```\n",
    "2. Carica ed esegue gli 11 classificatori, tenendo conto di un diverso numero di features (1, 9, 16, 30). Inoltre, ricava grandezze utili per valutare e confrontare i vari classificatori. Nello specifico:\n",
    "    - per ogni classificatore implementato con `sklearn` stampa su terminale i risultati relativi a:\n",
    "        1. matrice di confusione, grazie alla funzione `metrics.confusion_matrix`\n",
    "        2. report di classificazione, grazie alla funzione `metrics.classification_report`\n",
    "        3. accuratezza, grazie alla funzione `metrics.accuracy_score`.\n",
    "    - per il classificatore implementato con `pytorch` stampa su terminale i risultati relativi all'accuratezza, grazie alle righe di codice spiegate nella sezione **NNC**.\n",
    "    \n",
    "3. Salva in un DataFrame i risultati relativi alle accuratezze dei vari classificatori.\n",
    "\n",
    "4. Fa il plot delle funzioni di decisione per SVC nelle varie casistiche, prendendo in considerazione solo due features. Questo è implementato tramite la riga di codice:\n",
    "    ```python\n",
    "        Plotting.SVCPlot(s,e)\n",
    "    ```\n",
    "5. Permette di ottenere il Decision Tree, di esportarlo e salvarlo in un file pdf tramite la riga di codice:\n",
    "    ```python\n",
    "        Plotting.DTCPlot(s,e)\n",
    "    ```\n",
    "6. Realizza un istogramma monodimensionale raffigurante i coefficienti di correlazione di Spearman per ogni feature, in modo da individuare e visualizzare le 10 features migliori, tramite la riga di codice:\n",
    "    ```python\n",
    "        Plotting.Histo()\n",
    "    ```\n",
    "7. Carica ed esegue gli 11 classificatori, tenendo conto delle 10 features migliori. Stampa su terminale le stesse grandezze ottenute nel p.to 2.\n",
    "\n",
    "8. Salva in un DataFrame i risultati relativi alle accuratezze dei vari classificatori.\n",
    "\n",
    "9. Realizza un plot tridimensionale delle 3 features migliori per visualizzarne l'andamento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risultati e commenti finali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una volta fatto girare il programma, i risultati ottenuti per un training set dell'80% e un test set del 20% sono i seguenti:\n",
    "\n",
    "**1 feature (sx) e 9 features (dx)**\n",
    "\n",
    "|Classificatore|Errori Commessi|Percentuale letta correttamente|       |Classificatore|Errori Commessi|Percentuale letta correttamente| \n",
    "|:---|:---:|--:|       |:---|:---:|--:|\n",
    "|1. LogReg|10/114|91.23%|      |1. LogReg|9/114|92.11%|\n",
    "|2. SVM|9/114|92.11%|       |2. SVM|10/114|91.23%|\n",
    "|3. DTC|18/114|84.21%|       |3. DTC|8/114|92.98%|\n",
    "|4. KNC|15/114|86.84%|       |4. KNC|15/114|86.84%|\n",
    "|5. RFC|19/114|83.33%|       |5. RFC|5/114|95.61%|\n",
    "|6. MLP|12/114|89.47%|       |6. MLP|10/114|91.23%|\n",
    "|7. ABC|14/114|87.72%|       |7. ABC|6/114|94.74%|\n",
    "|8. GNB|12/114|89.47%|       |8. GNB|13/114|88.60%|\n",
    "|9. QDA|12/114|89.47%|       |9. QDA|6/114|94.74%|\n",
    "|10. SGD|13/114|88.60%|      |10. SGD|13/114|88.60%|\n",
    "|11. NNC|15/114|86.84%|      |11. NNC|6/114|94.74%|\n",
    "\n",
    "**16 features (sx) e 32 features (dx)**\n",
    "\n",
    "|Classificatore|Errori Commessi|Percentuale letta correttamente| |Classificatore|Errori Commessi|Percentuale letta correttamente| \n",
    "|--|:--:|--:| |--|:--:|--:|\n",
    "|1. LogReg|6/114|94.74%| |1. LogReg|5/114|95.61%|\n",
    "|2. SVM|8/114|92.98%| |2. SVM|5/114|95.61%|\n",
    "|3. DTC|9/114|92.11%| |3. DTC|10/114|91.23%|\n",
    "|4. KNC|12/114|89.47%| |4. KNC|10/114|91.23%|\n",
    "|5. RFC|8/114|92.98%| |5. RFC|5/114|95.61%|\n",
    "|6. MLP|7/114|93.86%| |6. MLP|11/114|90.35%|\n",
    "|7. ABC|5/114|95.61%| |7. ABC|5/114|95.61%|\n",
    "|8. GNB|11/114|90.35%| |8. GNB|6/114|92.98%|\n",
    "|9. QDA|5/114|95.61%| |9. QDA|5/114|95.61%|\n",
    "|10. SGD|19/114|83.33%| |10. SGD|19/114|83.33%|\n",
    "|11. NNC|5/114|95.61%| |11. NNC|7/114|93.86%|\n",
    "\n",
    "\n",
    "**10 features migliori**\n",
    "\n",
    "|Classificatore|Errori Commessi|Percentuale letta correttamente|\n",
    "|--|:--:|--:|\n",
    "|1. LogReg|7/114|93.86%|\n",
    "|2. SVM|8/114|92.98%|\n",
    "|3. DTC|7/114|93.86%|\n",
    "|4. KNC|9/114|92.11%|\n",
    "|5. RFC|5/114|95.61%|\n",
    "|6. MLP|10/114|91.23%|\n",
    "|7. ABC|4/114|96.49%|\n",
    "|8. GNB|8/114|92.98%|\n",
    "|9. QDA|7/114|93.86%|\n",
    "|10. SGD|14/114|87.72%|\n",
    "|11. NNC|6/114|94.74%|\n",
    "\n",
    "Osservando le tabelle è possibile notare come i classificatori migliori risultino:\n",
    "1. ABC che compare tra le performance migliori in presenza di 9,16,30 e 10 best ma non in 1 feature\n",
    "2. QDA che compare tra le performance migliori in presenza di 1,9,16 e 30 ma non in 10 best\n",
    "3. NNC che compare tra le performance migliori in presenza di 9,16,10 best ma non in 1,30 features.\n",
    "\n",
    "Bisogna specificare che il fatto che questi classificatori non compaiano tra le performance migliori in alcuni casi, non vuol dire che classifichino male ma che altri classificatori sono risultati migliori, a parità di condizioni. Infatti:\n",
    "1. ABC ha una performance dell' 87.72%, rispetto a SVM (92.11% = migliore performance) in presenza di 1 feature\n",
    "2. QDA ha una performane del 93.86%, rispetto a ABC (96.49% = migliore perfomance) in presenza di 10 best \n",
    "3. NNC ha una performance:\n",
    "    1. dell' 86.84% rispetto a SVM (92.11% = migliore performance) in presenza di 1 feature\n",
    "    2. del 93.86% rispetto a SVM (95.61% = migliore performance) in presenza di 30 features\n",
    "\n",
    "Inoltre, in generale si può notare come le performance dei classificatori migliorino significativamente all'aumentare del numero di features prese in considerazione durante la classificazione. Questo è in linea con quanto ci si aspetta teoricamente quando si parla di \"maledizione\" della dimensionalità.\n",
    "\n",
    "Il fatto che i 3 classificatori migliori siano ABC, QDA e NNC è spiegabile con:\n",
    "1. ABC e NNC sono classificatori adattativi. Inoltre, in generale, i classificatori basati su reti neurali sono la soluzione più rapida e diretta per avere performance consistenti;\n",
    "2. QDA si basa su una linea di decisione quadratica e sul teorema di Bayes. I classificatori bayesiani sono caratterizzati dalla minima probabilità di errore (Bayes Error Rate), ovvero migliore che un classificatore possa fare. Il fatto che GNB non lavori bene come QDA potrebbe essere legato alla minore flessibilità del primo modello in cui si fa anche un'assunzione di non correlazione tra le variabili all'interno di una stessa classe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questa sezione, sono mostrati per completezza di contenuti, i DataFrame con i dati relativi alle accuratezze dei classificatori per i vari splitting tra training e test.\n",
    "**Dataframe: 1 feature**\n",
    "<img src=\"files/img/df1.png\">\n",
    "**Dataframe: 9 features**\n",
    "<img src=\"files/img/df9.png\">\n",
    "**Dataframe: 16 features**\n",
    "<img src=\"files/img/df16.png\">\n",
    "**Dataframe: 30 features**\n",
    "<img src=\"files/img/df30.png\">\n",
    "**Dataframe: 10 features migliori**\n",
    "<img src=\"files/img/df10.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
